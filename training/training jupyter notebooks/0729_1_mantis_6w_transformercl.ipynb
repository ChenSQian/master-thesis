{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0729_1_transformercl_reproduce.ipynb","provenance":[{"file_id":"1MR6fqyFJ0F0CvZ_prOyG8prQRr9h1zwL","timestamp":1621518416179}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CgncrxE8FEY-"},"source":["function KeepClicking(){\n","console.log(\"Clicking\");\n","document.querySelector(\"colab-connect-button\").click()\n","}\n","setInterval(KeepClicking,60000)\n","Open your Chrome DevTools by pressing F12 or ctrl+shift+i on Linux and enter the following JavaScript snippet in your console:"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1QWVTurKCTMV","executionInfo":{"status":"ok","timestamp":1628681860328,"user_tz":-120,"elapsed":18287,"user":{"displayName":"T. Z.","photoUrl":"","userId":"04670299702794629529"}},"outputId":"d97d1dfc-82ce-4a85-c47c-e786c79e583a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0q-7RWqYrMbI"},"source":["# install transformers\n","!pip install pytorch_transformers\n","# Mount google drive\n","!pip install -r drive/MyDrive/transformers_cl/requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oJ0Oosmz89FN","executionInfo":{"status":"ok","timestamp":1628496482765,"user_tz":-120,"elapsed":182,"user":{"displayName":"Chen Qian","photoUrl":"","userId":"11245437508655380561"}},"outputId":"db6d557c-f4e3-4e69-d6cb-c74e0b093403"},"source":["%cd 'drive/MyDrive/transformers_cl'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/transformers_cl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F6mY4jG_4q2q"},"source":["from __future__ import absolute_import, division, print_function\n","from random import sample\n","import argparse\n","import glob\n","import logging\n","import os\n","import random\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n","                              TensorDataset, Subset)\n","from torch.utils.data.distributed import DistributedSampler\n","from tensorboardX import SummaryWriter\n","from tqdm import tqdm, trange\n","\n","from pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n","                                  BertForSequenceClassification, BertTokenizer,\n","                                  XLMConfig, XLMForSequenceClassification,\n","                                  XLMTokenizer, XLNetConfig,\n","                                  XLNetForSequenceClassification,\n","                                  XLNetTokenizer)\n","\n","from pytorch_transformers import AdamW, WarmupLinearSchedule\n","\n","from utils_glue import (compute_metrics, convert_examples_to_features,\n","                        output_modes, processors, read_curriculum_file, cycle,\n","                        compute_aps, read_scores_file)\n","\n","from pacing_functions import (PACING_FUNCTIONS)\n","from IPython import embed\n","from scipy.special import softmax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"70BWO3Dx83dc"},"source":["cached_features_file = \"../Mantis/cached_dev_bert-base-uncased_128_mantis_10\"\n","features = torch.load(cached_features_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P-GIHl4T9LQ7","executionInfo":{"status":"ok","timestamp":1628496553978,"user_tz":-120,"elapsed":191,"user":{"displayName":"Chen Qian","photoUrl":"","userId":"11245437508655380561"}},"outputId":"1d51c877-4ae6-40d8-9d57-7d8f96d3b546"},"source":["import numpy as np\n","np.shape(features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(180960,)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5F1ZFmho9XJU","executionInfo":{"status":"ok","timestamp":1628496640832,"user_tz":-120,"elapsed":196,"user":{"displayName":"Chen Qian","photoUrl":"","userId":"11245437508655380561"}},"outputId":"377647ec-04af-4f31-d439-9a65a2dc4597"},"source":["sample(features, 10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<utils_glue.InputFeatures at 0x7fb8d9b32a10>,\n"," <utils_glue.InputFeatures at 0x7fb8b9990b50>,\n"," <utils_glue.InputFeatures at 0x7fb8bdcca650>,\n"," <utils_glue.InputFeatures at 0x7fb8d2b21050>,\n"," <utils_glue.InputFeatures at 0x7fb8b4174f50>,\n"," <utils_glue.InputFeatures at 0x7fb8cd7832d0>,\n"," <utils_glue.InputFeatures at 0x7fb8d5866310>,\n"," <utils_glue.InputFeatures at 0x7fb8b7323490>,\n"," <utils_glue.InputFeatures at 0x7fb8d7b41410>,\n"," <utils_glue.InputFeatures at 0x7fb8c7afb890>]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"xw2TAOpR3rQK"},"source":["# Mantis root-2"]},{"cell_type":"code","metadata":{"id":"0TERGUBVl5VN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628551723382,"user_tz":-120,"elapsed":8347361,"user":{"displayName":"Chen Qian","photoUrl":"","userId":"11245437508655380561"}},"outputId":"cd4693a5-eab0-4407-e65f-b266445092ca"},"source":["# seed 1 60000\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 1\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_root2_seed_1_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function root_2\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/09/2021 21:09:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/09/2021 21:09:41 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpf527geb_\n","100% 433/433 [00:00<00:00, 440274.82B/s]\n","08/09/2021 21:09:41 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpf527geb_ to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 21:09:41 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 21:09:41 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpf527geb_\n","08/09/2021 21:09:41 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 21:09:41 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/09/2021 21:09:42 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp_z2j2dkd\n","100% 231508/231508 [00:00<00:00, 872406.01B/s]\n","08/09/2021 21:09:42 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp_z2j2dkd to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 21:09:42 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 21:09:42 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp_z2j2dkd\n","08/09/2021 21:09:42 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 21:09:42 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp104xs9ha\n","100% 440473133/440473133 [00:12<00:00, 36069919.41B/s]\n","08/09/2021 21:09:55 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp104xs9ha to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 21:09:56 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 21:09:56 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp104xs9ha\n","08/09/2021 21:09:56 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 21:09:59 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/09/2021 21:09:59 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/09/2021 21:10:11 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_root2_seed_1_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='root_2', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_1', save_aps=False, save_steps=5000, seed=1, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/09/2021 21:10:11 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/09/2021 21:10:33 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/09/2021 21:10:35 - INFO - __main__ -   ***** Running training *****\n","08/09/2021 21:10:35 - INFO - __main__ -     Num examples = 164544\n","08/09/2021 21:10:35 - INFO - __main__ -     Num Epochs = 1\n","08/09/2021 21:10:35 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/09/2021 21:10:35 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/09/2021 21:10:35 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/09/2021 21:10:35 - INFO - __main__ -     Total optimization steps = 2571\n","08/09/2021 21:10:35 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/09/2021 21:10:35 - INFO - __main__ -     data_loaders = [('pacing_function_root_2', <torch.utils.data.dataloader.DataLoader object at 0x7f30ab885910>)]\n","08/09/2021 21:10:35 - INFO - __main__ -   Starting epoch 1\n","08/09/2021 21:10:35 - INFO - __main__ -   Training with pacing_function_root_2\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/09/2021 21:18:41 - INFO - __main__ -   Iter = 300\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/09/2021 21:18:41 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/09/2021 21:18:41 - INFO - __main__ -   loss = 0.21472005521257717\n","08/09/2021 21:18:41 - INFO - __main__ -   Current data iter size: 1217\n","08/09/2021 21:18:41 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 21:19:03 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 21:19:03 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 21:19:03 - INFO - __main__ -     Batch size = 64\n","08/09/2021 21:27:34 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 21:27:34 - INFO - __main__ -     map = 0.7368665306728499\n","08/09/2021 21:27:36 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_1\n","08/09/2021 21:35:44 - INFO - __main__ -   Iter = 600\n","08/09/2021 21:35:44 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/09/2021 21:35:44 - INFO - __main__ -   loss = 0.1939725836366415\n","08/09/2021 21:35:44 - INFO - __main__ -   Current data iter size: 1499\n","08/09/2021 21:35:44 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 21:36:02 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 21:36:02 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 21:36:02 - INFO - __main__ -     Batch size = 64\n","08/09/2021 21:44:33 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 21:44:33 - INFO - __main__ -     map = 0.7479093887380953\n","08/09/2021 21:44:35 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_1\n","08/09/2021 21:52:44 - INFO - __main__ -   Iter = 900\n","08/09/2021 21:52:44 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/09/2021 21:52:44 - INFO - __main__ -   loss = 0.22766262955963612\n","08/09/2021 21:52:44 - INFO - __main__ -   Current data iter size: 1735\n","08/09/2021 21:52:44 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 21:53:02 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 21:53:02 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 21:53:02 - INFO - __main__ -     Batch size = 64\n","08/09/2021 22:01:32 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 22:01:32 - INFO - __main__ -     map = 0.7563500266822221\n","08/09/2021 22:01:34 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_1\n","08/09/2021 22:09:44 - INFO - __main__ -   Iter = 1200\n","08/09/2021 22:09:44 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/09/2021 22:09:44 - INFO - __main__ -   loss = 0.2823120153943698\n","08/09/2021 22:09:44 - INFO - __main__ -   Current data iter size: 1943\n","08/09/2021 22:09:44 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 22:10:02 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 22:10:02 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 22:10:02 - INFO - __main__ -     Batch size = 64\n","08/09/2021 22:18:34 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 22:18:34 - INFO - __main__ -     map = 0.7622156658252833\n","08/09/2021 22:18:36 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_1\n","08/09/2021 22:26:45 - INFO - __main__ -   Iter = 1500\n","08/09/2021 22:26:45 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/09/2021 22:26:45 - INFO - __main__ -   loss = 0.33551425755023956\n","08/09/2021 22:26:45 - INFO - __main__ -   Current data iter size: 2130\n","08/09/2021 22:26:45 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 22:27:03 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 22:27:03 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 22:27:03 - INFO - __main__ -     Batch size = 64\n","08/09/2021 22:35:35 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 22:35:35 - INFO - __main__ -     map = 0.7596566761389328\n","08/09/2021 22:43:45 - INFO - __main__ -   Iter = 1800\n","08/09/2021 22:43:45 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/09/2021 22:43:45 - INFO - __main__ -   loss = 0.3819956675171852\n","08/09/2021 22:43:45 - INFO - __main__ -   Current data iter size: 2303\n","08/09/2021 22:43:45 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 22:44:03 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 22:44:03 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 22:44:03 - INFO - __main__ -     Batch size = 64\n","08/09/2021 22:52:35 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 22:52:35 - INFO - __main__ -     map = 0.7646653913183327\n","08/09/2021 22:52:37 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_1\n","08/09/2021 23:00:47 - INFO - __main__ -   Iter = 2100\n","08/09/2021 23:00:47 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/09/2021 23:00:47 - INFO - __main__ -   loss = 0.41788871213793755\n","08/09/2021 23:00:47 - INFO - __main__ -   Current data iter size: 2463\n","08/09/2021 23:00:47 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 23:01:05 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 23:01:05 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 23:01:05 - INFO - __main__ -     Batch size = 64\n","08/09/2021 23:09:36 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 23:09:36 - INFO - __main__ -     map = 0.7648344997837874\n","08/09/2021 23:09:38 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_1\n","08/09/2021 23:17:48 - INFO - __main__ -   Iter = 2400\n","08/09/2021 23:17:48 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/09/2021 23:17:48 - INFO - __main__ -   loss = 0.45386710653702417\n","08/09/2021 23:17:48 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 23:17:48 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 23:18:05 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 23:18:05 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 23:18:05 - INFO - __main__ -     Batch size = 64\n","08/09/2021 23:26:37 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 23:26:37 - INFO - __main__ -     map = 0.7729837501470761\n","08/09/2021 23:26:39 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_1\n","Traceback (most recent call last):\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 704, in <module>\n","    main()\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 651, in main\n","    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 227, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 255, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 149, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EC44heWV1P3n","executionInfo":{"status":"ok","timestamp":1628611146821,"user_tz":-120,"elapsed":8378301,"user":{"displayName":"xiaokang qian","photoUrl":"","userId":"16136989389156139825"}},"outputId":"5681a8de-1187-403f-843e-90b7ef50b909"},"source":["# seed 2 60000\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 2\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_root2_seed_2_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function root_2\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/10/2021 13:39:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/10/2021 13:39:34 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp6p9ags_1\n","100% 433/433 [00:00<00:00, 420206.76B/s]\n","08/10/2021 13:39:34 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp6p9ags_1 to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 13:39:34 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 13:39:34 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp6p9ags_1\n","08/10/2021 13:39:34 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 13:39:34 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/10/2021 13:39:34 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp4aevnr9n\n","100% 231508/231508 [00:00<00:00, 913945.44B/s]\n","08/10/2021 13:39:35 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp4aevnr9n to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 13:39:35 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 13:39:35 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp4aevnr9n\n","08/10/2021 13:39:35 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 13:39:35 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp4gbx9i8w\n","100% 440473133/440473133 [00:10<00:00, 40189949.04B/s]\n","08/10/2021 13:39:46 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp4gbx9i8w to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 13:39:47 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 13:39:47 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp4gbx9i8w\n","08/10/2021 13:39:47 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 13:39:50 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/10/2021 13:39:50 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/10/2021 13:40:02 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_root2_seed_2_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='root_2', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_2', save_aps=False, save_steps=5000, seed=2, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/10/2021 13:40:02 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/10/2021 13:40:23 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/10/2021 13:40:26 - INFO - __main__ -   ***** Running training *****\n","08/10/2021 13:40:26 - INFO - __main__ -     Num examples = 164544\n","08/10/2021 13:40:26 - INFO - __main__ -     Num Epochs = 1\n","08/10/2021 13:40:26 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/10/2021 13:40:26 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/10/2021 13:40:26 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/10/2021 13:40:26 - INFO - __main__ -     Total optimization steps = 2571\n","08/10/2021 13:40:26 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/10/2021 13:40:26 - INFO - __main__ -     data_loaders = [('pacing_function_root_2', <torch.utils.data.dataloader.DataLoader object at 0x7f77b8388950>)]\n","08/10/2021 13:40:26 - INFO - __main__ -   Starting epoch 1\n","08/10/2021 13:40:26 - INFO - __main__ -   Training with pacing_function_root_2\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/10/2021 13:48:11 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/10/2021 13:48:11 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/10/2021 13:48:11 - INFO - __main__ -   loss = 0.2422825379172961\n","08/10/2021 13:48:11 - INFO - __main__ -   Current data iter size: 1217\n","08/10/2021 13:48:11 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 13:48:33 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 13:48:33 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 13:48:33 - INFO - __main__ -     Batch size = 64\n","08/10/2021 13:56:57 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 13:56:57 - INFO - __main__ -     map = 0.721178672015201\n","08/10/2021 13:56:59 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_2\n","08/10/2021 14:05:03 - INFO - __main__ -   Iter = 600.0\n","08/10/2021 14:05:03 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/10/2021 14:05:03 - INFO - __main__ -   loss = 0.1993149218087395\n","08/10/2021 14:05:03 - INFO - __main__ -   Current data iter size: 1499\n","08/10/2021 14:05:03 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 14:05:20 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 14:05:20 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 14:05:20 - INFO - __main__ -     Batch size = 64\n","08/10/2021 14:13:45 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 14:13:45 - INFO - __main__ -     map = 0.7395000450769832\n","08/10/2021 14:13:47 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_2\n","08/10/2021 14:21:51 - INFO - __main__ -   Iter = 900.0\n","08/10/2021 14:21:51 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/10/2021 14:21:51 - INFO - __main__ -   loss = 0.23598814385632674\n","08/10/2021 14:21:51 - INFO - __main__ -   Current data iter size: 1735\n","08/10/2021 14:21:51 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 14:22:07 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 14:22:07 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 14:22:07 - INFO - __main__ -     Batch size = 64\n","08/10/2021 14:30:31 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 14:30:31 - INFO - __main__ -     map = 0.7489030350122258\n","08/10/2021 14:30:33 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_2\n","08/10/2021 14:38:37 - INFO - __main__ -   Iter = 1200.0\n","08/10/2021 14:38:37 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/10/2021 14:38:37 - INFO - __main__ -   loss = 0.28601097335418063\n","08/10/2021 14:38:37 - INFO - __main__ -   Current data iter size: 1943\n","08/10/2021 14:38:37 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 14:38:54 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 14:38:54 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 14:38:54 - INFO - __main__ -     Batch size = 64\n","08/10/2021 14:47:18 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 14:47:18 - INFO - __main__ -     map = 0.7562288686795576\n","08/10/2021 14:47:19 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_2\n","08/10/2021 14:55:23 - INFO - __main__ -   Iter = 1500.0\n","08/10/2021 14:55:23 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/10/2021 14:55:23 - INFO - __main__ -   loss = 0.3372109246253967\n","08/10/2021 14:55:23 - INFO - __main__ -   Current data iter size: 2130\n","08/10/2021 14:55:23 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 14:55:40 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 14:55:40 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 14:55:40 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:04:03 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:04:03 - INFO - __main__ -     map = 0.7509656164479118\n","08/10/2021 15:12:07 - INFO - __main__ -   Iter = 1800.0\n","08/10/2021 15:12:07 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/10/2021 15:12:07 - INFO - __main__ -   loss = 0.38898201028505963\n","08/10/2021 15:12:07 - INFO - __main__ -   Current data iter size: 2303\n","08/10/2021 15:12:07 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 15:12:23 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 15:12:23 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 15:12:23 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:20:48 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:20:48 - INFO - __main__ -     map = 0.7602066322924227\n","08/10/2021 15:20:50 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_2\n","08/10/2021 15:28:55 - INFO - __main__ -   Iter = 2100.0\n","08/10/2021 15:28:55 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/10/2021 15:28:55 - INFO - __main__ -   loss = 0.4294292871157328\n","08/10/2021 15:28:55 - INFO - __main__ -   Current data iter size: 2463\n","08/10/2021 15:28:55 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 15:29:11 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 15:29:11 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 15:29:11 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:37:37 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:37:37 - INFO - __main__ -     map = 0.758213973515816\n","08/10/2021 15:45:42 - INFO - __main__ -   Iter = 2400.0\n","08/10/2021 15:45:42 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/10/2021 15:45:42 - INFO - __main__ -   loss = 0.4638959621389707\n","08/10/2021 15:45:42 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 15:45:42 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 15:46:00 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 15:46:00 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 15:46:00 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:54:24 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:54:24 - INFO - __main__ -     map = 0.7670688947728366\n","08/10/2021 15:54:25 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_2\n","08/10/2021 15:59:04 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/10/2021 15:59:04 - INFO - __main__ -    global_step = 2572.0, average loss = 0.3329866443678134\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAbBXoz2rVx6","executionInfo":{"status":"ok","timestamp":1628634119678,"user_tz":-120,"elapsed":8060331,"user":{"displayName":"T. Z.","photoUrl":"","userId":"04670299702794629529"}},"outputId":"a7b50919-2263-4b57-da3c-18e80761b798"},"source":["!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 3\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_root2_seed_3_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function root_2\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/10/2021 20:07:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/10/2021 20:07:45 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpucng8vzu\n","100% 433/433 [00:00<00:00, 513757.75B/s]\n","08/10/2021 20:07:46 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpucng8vzu to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:07:46 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:07:46 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpucng8vzu\n","08/10/2021 20:07:46 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:07:46 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/10/2021 20:07:46 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpck897fig\n","100% 231508/231508 [00:00<00:00, 1235943.03B/s]\n","08/10/2021 20:07:46 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpck897fig to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:07:46 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:07:46 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpck897fig\n","08/10/2021 20:07:46 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:07:47 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpy_gcz3ol\n","100% 440473133/440473133 [00:11<00:00, 38143512.88B/s]\n","08/10/2021 20:07:58 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpy_gcz3ol to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:08:00 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:08:00 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpy_gcz3ol\n","08/10/2021 20:08:00 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:08:03 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/10/2021 20:08:03 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/10/2021 20:08:14 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_root2_seed_3_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='root_2', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_3', save_aps=False, save_steps=5000, seed=3, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/10/2021 20:08:14 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/10/2021 20:08:36 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/10/2021 20:08:39 - INFO - __main__ -   ***** Running training *****\n","08/10/2021 20:08:39 - INFO - __main__ -     Num examples = 164544\n","08/10/2021 20:08:39 - INFO - __main__ -     Num Epochs = 1\n","08/10/2021 20:08:39 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/10/2021 20:08:39 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/10/2021 20:08:39 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/10/2021 20:08:39 - INFO - __main__ -     Total optimization steps = 2571\n","08/10/2021 20:08:39 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/10/2021 20:08:39 - INFO - __main__ -     data_loaders = [('pacing_function_root_2', <torch.utils.data.dataloader.DataLoader object at 0x7f06e6da0810>)]\n","08/10/2021 20:08:39 - INFO - __main__ -   Starting epoch 1\n","08/10/2021 20:08:39 - INFO - __main__ -   Training with pacing_function_root_2\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/10/2021 20:16:18 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/10/2021 20:16:18 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/10/2021 20:16:18 - INFO - __main__ -   loss = 0.2365995483348767\n","08/10/2021 20:16:18 - INFO - __main__ -   Current data iter size: 1217\n","08/10/2021 20:16:18 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:16:39 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:16:39 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:16:39 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:24:43 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:24:43 - INFO - __main__ -     map = 0.7292256414095015\n","08/10/2021 20:24:45 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_3\n","08/10/2021 20:32:31 - INFO - __main__ -   Iter = 600.0\n","08/10/2021 20:32:31 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/10/2021 20:32:31 - INFO - __main__ -   loss = 0.1916048813983798\n","08/10/2021 20:32:31 - INFO - __main__ -   Current data iter size: 1499\n","08/10/2021 20:32:31 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:32:48 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:32:48 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:32:48 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:40:49 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:40:49 - INFO - __main__ -     map = 0.7479211092533777\n","08/10/2021 20:40:51 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_3\n","08/10/2021 20:48:34 - INFO - __main__ -   Iter = 900.0\n","08/10/2021 20:48:34 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/10/2021 20:48:34 - INFO - __main__ -   loss = 0.23079622944196065\n","08/10/2021 20:48:34 - INFO - __main__ -   Current data iter size: 1735\n","08/10/2021 20:48:34 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:48:50 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:48:50 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:48:50 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:56:47 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:56:47 - INFO - __main__ -     map = 0.7532426479263535\n","08/10/2021 20:56:49 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_3\n","08/10/2021 21:04:35 - INFO - __main__ -   Iter = 1200.0\n","08/10/2021 21:04:35 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/10/2021 21:04:35 - INFO - __main__ -   loss = 0.28574087013800936\n","08/10/2021 21:04:35 - INFO - __main__ -   Current data iter size: 1943\n","08/10/2021 21:04:35 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:04:53 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:04:53 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:04:53 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:12:58 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:12:58 - INFO - __main__ -     map = 0.7509725817559982\n","08/10/2021 21:20:43 - INFO - __main__ -   Iter = 1500.0\n","08/10/2021 21:20:43 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/10/2021 21:20:43 - INFO - __main__ -   loss = 0.3285297042628129\n","08/10/2021 21:20:43 - INFO - __main__ -   Current data iter size: 2130\n","08/10/2021 21:20:43 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:21:00 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:21:00 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:21:00 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:28:58 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:28:58 - INFO - __main__ -     map = 0.7604577035443776\n","08/10/2021 21:28:59 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_3\n","08/10/2021 21:36:47 - INFO - __main__ -   Iter = 1800.0\n","08/10/2021 21:36:47 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/10/2021 21:36:47 - INFO - __main__ -   loss = 0.38237027660012246\n","08/10/2021 21:36:47 - INFO - __main__ -   Current data iter size: 2303\n","08/10/2021 21:36:47 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:37:04 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:37:04 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:37:04 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:45:09 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:45:09 - INFO - __main__ -     map = 0.762714513491231\n","08/10/2021 21:45:10 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_3\n","08/10/2021 21:52:59 - INFO - __main__ -   Iter = 2100.0\n","08/10/2021 21:52:59 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/10/2021 21:52:59 - INFO - __main__ -   loss = 0.41077290455500287\n","08/10/2021 21:52:59 - INFO - __main__ -   Current data iter size: 2463\n","08/10/2021 21:52:59 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:53:15 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:53:15 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:53:15 - INFO - __main__ -     Batch size = 64\n","08/10/2021 22:01:20 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 22:01:20 - INFO - __main__ -     map = 0.771596056473798\n","08/10/2021 22:01:22 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root2_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_2_seed_3\n","08/10/2021 22:09:10 - INFO - __main__ -   Iter = 2400.0\n","08/10/2021 22:09:10 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/10/2021 22:09:10 - INFO - __main__ -   loss = 0.4629849427938461\n","08/10/2021 22:09:10 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 22:09:10 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 22:09:26 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 22:09:26 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 22:09:26 - INFO - __main__ -     Batch size = 64\n","08/10/2021 22:17:31 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 22:17:31 - INFO - __main__ -     map = 0.7635886814580392\n","08/10/2021 22:21:58 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/10/2021 22:21:58 - INFO - __main__ -    global_step = 2572.0, average loss = 0.3266211559005302\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AvEIlYv8kZeb"},"source":["# Mantis root-5"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SO4JVs1rIHp8","executionInfo":{"status":"ok","timestamp":1628525289955,"user_tz":-120,"elapsed":9116183,"user":{"displayName":"xiaokang qian","photoUrl":"","userId":"16136989389156139825"}},"outputId":"6b32ba27-e6d6-4d9d-eb6f-cedcd3566471"},"source":["# seed 1 6w\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 1\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_root5_seed_1_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function root_5\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/09/2021 13:36:19 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/09/2021 13:36:19 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpcb7_b74e\n","100% 433/433 [00:00<00:00, 406839.97B/s]\n","08/09/2021 13:36:20 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpcb7_b74e to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 13:36:20 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 13:36:20 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpcb7_b74e\n","08/09/2021 13:36:20 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 13:36:20 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/09/2021 13:36:20 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmphvckobr7\n","100% 231508/231508 [00:00<00:00, 930752.44B/s]\n","08/09/2021 13:36:20 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmphvckobr7 to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 13:36:20 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 13:36:20 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmphvckobr7\n","08/09/2021 13:36:20 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 13:36:21 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp1r5gqbav\n","100% 440473133/440473133 [00:14<00:00, 31332665.87B/s]\n","08/09/2021 13:36:35 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp1r5gqbav to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 13:36:36 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 13:36:36 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp1r5gqbav\n","08/09/2021 13:36:36 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 13:36:39 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/09/2021 13:36:39 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/09/2021 13:36:51 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_root5_seed_1_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='root_5', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_1', save_aps=False, save_steps=5000, seed=1, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/09/2021 13:36:51 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/09/2021 13:37:13 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/09/2021 13:37:16 - INFO - __main__ -   ***** Running training *****\n","08/09/2021 13:37:16 - INFO - __main__ -     Num examples = 164544\n","08/09/2021 13:37:16 - INFO - __main__ -     Num Epochs = 1\n","08/09/2021 13:37:16 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/09/2021 13:37:16 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/09/2021 13:37:16 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/09/2021 13:37:16 - INFO - __main__ -     Total optimization steps = 2571\n","08/09/2021 13:37:16 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/09/2021 13:37:16 - INFO - __main__ -     data_loaders = [('pacing_function_root_5', <torch.utils.data.dataloader.DataLoader object at 0x7f55f79be050>)]\n","08/09/2021 13:37:16 - INFO - __main__ -   Starting epoch 1\n","08/09/2021 13:37:16 - INFO - __main__ -   Training with pacing_function_root_5\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/09/2021 13:45:57 - INFO - __main__ -   Iter = 300\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/09/2021 13:45:57 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/09/2021 13:45:57 - INFO - __main__ -   loss = 0.32772815237442654\n","08/09/2021 13:45:57 - INFO - __main__ -   Current data iter size: 1717\n","08/09/2021 13:45:57 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 13:46:17 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 13:46:17 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 13:46:17 - INFO - __main__ -     Batch size = 64\n","08/09/2021 13:55:27 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 13:55:27 - INFO - __main__ -     map = 0.7450846615146219\n","08/09/2021 13:55:29 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_1\n","08/09/2021 14:04:13 - INFO - __main__ -   Iter = 600\n","08/09/2021 14:04:13 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/09/2021 14:04:13 - INFO - __main__ -   loss = 0.3296228434642156\n","08/09/2021 14:04:13 - INFO - __main__ -   Current data iter size: 1967\n","08/09/2021 14:04:13 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:04:30 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:04:30 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:04:30 - INFO - __main__ -     Batch size = 64\n","08/09/2021 14:13:42 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 14:13:42 - INFO - __main__ -     map = 0.7593249247123631\n","08/09/2021 14:13:43 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_1\n","08/09/2021 14:22:28 - INFO - __main__ -   Iter = 900\n","08/09/2021 14:22:28 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/09/2021 14:22:28 - INFO - __main__ -   loss = 0.35819009944796565\n","08/09/2021 14:22:28 - INFO - __main__ -   Current data iter size: 2131\n","08/09/2021 14:22:28 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:22:45 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:22:45 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:22:45 - INFO - __main__ -     Batch size = 64\n","08/09/2021 14:31:56 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 14:31:56 - INFO - __main__ -     map = 0.7567296140123944\n","08/09/2021 14:40:40 - INFO - __main__ -   Iter = 1200\n","08/09/2021 14:40:40 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/09/2021 14:40:40 - INFO - __main__ -   loss = 0.388465743213892\n","08/09/2021 14:40:40 - INFO - __main__ -   Current data iter size: 2256\n","08/09/2021 14:40:40 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:40:58 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:40:58 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:40:58 - INFO - __main__ -     Batch size = 64\n","08/09/2021 14:50:07 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 14:50:07 - INFO - __main__ -     map = 0.7650768280788277\n","08/09/2021 14:50:08 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_1\n","08/09/2021 14:58:53 - INFO - __main__ -   Iter = 1500\n","08/09/2021 14:58:53 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/09/2021 14:58:53 - INFO - __main__ -   loss = 0.407425283541282\n","08/09/2021 14:58:53 - INFO - __main__ -   Current data iter size: 2359\n","08/09/2021 14:58:53 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:59:09 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:59:09 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:59:09 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:08:20 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:08:20 - INFO - __main__ -     map = 0.7555826192249862\n","08/09/2021 15:17:05 - INFO - __main__ -   Iter = 1800\n","08/09/2021 15:17:05 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/09/2021 15:17:05 - INFO - __main__ -   loss = 0.41817535931865374\n","08/09/2021 15:17:05 - INFO - __main__ -   Current data iter size: 2446\n","08/09/2021 15:17:05 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:17:22 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:17:22 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:17:22 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:26:34 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:26:34 - INFO - __main__ -     map = 0.7694947143267845\n","08/09/2021 15:26:35 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_1\n","08/09/2021 15:35:21 - INFO - __main__ -   Iter = 2100\n","08/09/2021 15:35:21 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/09/2021 15:35:21 - INFO - __main__ -   loss = 0.43700754895806315\n","08/09/2021 15:35:21 - INFO - __main__ -   Current data iter size: 2522\n","08/09/2021 15:35:21 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:35:39 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:35:39 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:35:39 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:44:50 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:44:50 - INFO - __main__ -     map = 0.7663872888543362\n","08/09/2021 15:53:36 - INFO - __main__ -   Iter = 2400\n","08/09/2021 15:53:36 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/09/2021 15:53:36 - INFO - __main__ -   loss = 0.45793309072653454\n","08/09/2021 15:53:36 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 15:53:36 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:53:53 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:53:53 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:53:53 - INFO - __main__ -     Batch size = 64\n","08/09/2021 16:03:05 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 16:03:05 - INFO - __main__ -     map = 0.7765435858387222\n","08/09/2021 16:03:06 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_1\n","08/09/2021 16:08:07 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/09/2021 16:08:08 - INFO - __main__ -    global_step = 2572, average loss = 0.39572331769910424\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6fjeyQdxkyjv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628614155770,"user_tz":-120,"elapsed":8403511,"user":{"displayName":"xy GG","photoUrl":"","userId":"06169222138223251748"}},"outputId":"ec4b4825-952e-46b9-aa18-b582fa25f8a4"},"source":["# seed 1 6 h for 2 epochs\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 2\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_root5_seed_2_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function root_5\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/10/2021 14:29:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/10/2021 14:29:18 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpikzo_bn5\n","100% 433/433 [00:00<00:00, 444912.70B/s]\n","08/10/2021 14:29:18 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpikzo_bn5 to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 14:29:18 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 14:29:18 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpikzo_bn5\n","08/10/2021 14:29:18 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 14:29:18 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/10/2021 14:29:18 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp2bx20ak7\n","100% 231508/231508 [00:00<00:00, 872181.89B/s]\n","08/10/2021 14:29:19 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp2bx20ak7 to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 14:29:19 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 14:29:19 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp2bx20ak7\n","08/10/2021 14:29:19 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 14:29:19 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp5ikjuqjo\n","100% 440473133/440473133 [00:11<00:00, 38091088.10B/s]\n","08/10/2021 14:29:31 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp5ikjuqjo to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 14:29:32 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 14:29:32 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp5ikjuqjo\n","08/10/2021 14:29:32 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 14:29:35 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/10/2021 14:29:35 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/10/2021 14:29:48 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_root5_seed_2_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='root_5', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_2', save_aps=False, save_steps=5000, seed=2, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/10/2021 14:29:48 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/10/2021 14:30:08 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/10/2021 14:30:11 - INFO - __main__ -   ***** Running training *****\n","08/10/2021 14:30:11 - INFO - __main__ -     Num examples = 164544\n","08/10/2021 14:30:11 - INFO - __main__ -     Num Epochs = 1\n","08/10/2021 14:30:11 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/10/2021 14:30:11 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/10/2021 14:30:11 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/10/2021 14:30:11 - INFO - __main__ -     Total optimization steps = 2571\n","08/10/2021 14:30:11 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/10/2021 14:30:11 - INFO - __main__ -     data_loaders = [('pacing_function_root_5', <torch.utils.data.dataloader.DataLoader object at 0x7f67345a1a50>)]\n","08/10/2021 14:30:11 - INFO - __main__ -   Starting epoch 1\n","08/10/2021 14:30:11 - INFO - __main__ -   Training with pacing_function_root_5\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/10/2021 14:38:13 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/10/2021 14:38:13 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/10/2021 14:38:13 - INFO - __main__ -   loss = 0.3560441493988037\n","08/10/2021 14:38:13 - INFO - __main__ -   Current data iter size: 1717\n","08/10/2021 14:38:13 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 14:38:34 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 14:38:34 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 14:38:34 - INFO - __main__ -     Batch size = 64\n","08/10/2021 14:46:59 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 14:46:59 - INFO - __main__ -     map = 0.719598065069532\n","08/10/2021 14:47:01 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_2\n","08/10/2021 14:55:04 - INFO - __main__ -   Iter = 600.0\n","08/10/2021 14:55:04 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/10/2021 14:55:04 - INFO - __main__ -   loss = 0.3418923469384511\n","08/10/2021 14:55:04 - INFO - __main__ -   Current data iter size: 1967\n","08/10/2021 14:55:04 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 14:55:21 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 14:55:21 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 14:55:21 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:03:47 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:03:47 - INFO - __main__ -     map = 0.7430453317570542\n","08/10/2021 15:03:48 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_2\n","08/10/2021 15:11:52 - INFO - __main__ -   Iter = 900.0\n","08/10/2021 15:11:52 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/10/2021 15:11:52 - INFO - __main__ -   loss = 0.37324042052030565\n","08/10/2021 15:11:52 - INFO - __main__ -   Current data iter size: 2131\n","08/10/2021 15:11:52 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 15:12:08 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 15:12:08 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 15:12:08 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:20:33 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:20:33 - INFO - __main__ -     map = 0.7530816905543902\n","08/10/2021 15:20:35 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_2\n","08/10/2021 15:28:38 - INFO - __main__ -   Iter = 1200.0\n","08/10/2021 15:28:38 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/10/2021 15:28:38 - INFO - __main__ -   loss = 0.3958135745426019\n","08/10/2021 15:28:38 - INFO - __main__ -   Current data iter size: 2256\n","08/10/2021 15:28:38 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 15:28:56 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 15:28:56 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 15:28:56 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:37:22 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:37:22 - INFO - __main__ -     map = 0.7619807821316769\n","08/10/2021 15:37:23 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_2\n","08/10/2021 15:45:28 - INFO - __main__ -   Iter = 1500.0\n","08/10/2021 15:45:28 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/10/2021 15:45:28 - INFO - __main__ -   loss = 0.41044955685734746\n","08/10/2021 15:45:28 - INFO - __main__ -   Current data iter size: 2359\n","08/10/2021 15:45:28 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 15:45:45 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 15:45:45 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 15:45:45 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:54:10 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:54:10 - INFO - __main__ -     map = 0.7517427842552796\n","08/10/2021 16:02:15 - INFO - __main__ -   Iter = 1800.0\n","08/10/2021 16:02:15 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/10/2021 16:02:15 - INFO - __main__ -   loss = 0.432193064391613\n","08/10/2021 16:02:15 - INFO - __main__ -   Current data iter size: 2446\n","08/10/2021 16:02:15 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 16:02:31 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 16:02:31 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 16:02:31 - INFO - __main__ -     Batch size = 64\n","08/10/2021 16:10:57 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 16:10:57 - INFO - __main__ -     map = 0.7597297044417917\n","08/10/2021 16:19:02 - INFO - __main__ -   Iter = 2100.0\n","08/10/2021 16:19:02 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/10/2021 16:19:02 - INFO - __main__ -   loss = 0.4452184976140658\n","08/10/2021 16:19:02 - INFO - __main__ -   Current data iter size: 2522\n","08/10/2021 16:19:02 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 16:19:19 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 16:19:19 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 16:19:19 - INFO - __main__ -     Batch size = 64\n","08/10/2021 16:27:45 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 16:27:45 - INFO - __main__ -     map = 0.7571308693977372\n","08/10/2021 16:35:50 - INFO - __main__ -   Iter = 2400.0\n","08/10/2021 16:35:50 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/10/2021 16:35:50 - INFO - __main__ -   loss = 0.46352751592795055\n","08/10/2021 16:35:50 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 16:35:50 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 16:36:07 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 16:36:07 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 16:36:07 - INFO - __main__ -     Batch size = 64\n","08/10/2021 16:44:33 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 16:44:33 - INFO - __main__ -     map = 0.767539442428547\n","08/10/2021 16:44:35 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_2\n","08/10/2021 16:49:13 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/10/2021 16:49:13 - INFO - __main__ -    global_step = 2572.0, average loss = 0.4067529908397335\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIDrfH9YOgmx","executionInfo":{"status":"ok","timestamp":1628681450739,"user_tz":-120,"elapsed":8506372,"user":{"displayName":"Chen Qian","photoUrl":"","userId":"11245437508655380561"}},"outputId":"0c3943a5-9aeb-4ced-b34f-bc46cb6db4e0"},"source":["# seed 2 60000\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 3\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_root5_seed_3_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function root_5\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/11/2021 09:09:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/11/2021 09:09:12 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp4iw4_nlc\n","100% 433/433 [00:00<00:00, 468437.87B/s]\n","08/11/2021 09:09:13 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp4iw4_nlc to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 09:09:13 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 09:09:13 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp4iw4_nlc\n","08/11/2021 09:09:13 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 09:09:13 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/11/2021 09:09:13 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpfw1wnx28\n","100% 231508/231508 [00:00<00:00, 841525.96B/s]\n","08/11/2021 09:09:13 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpfw1wnx28 to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 09:09:13 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 09:09:13 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpfw1wnx28\n","08/11/2021 09:09:13 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 09:09:14 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpnrv3p8i2\n","100% 440473133/440473133 [00:12<00:00, 34576184.62B/s]\n","08/11/2021 09:09:27 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpnrv3p8i2 to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 09:09:28 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 09:09:28 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpnrv3p8i2\n","08/11/2021 09:09:28 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 09:09:31 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/11/2021 09:09:31 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/11/2021 09:09:43 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_root5_seed_3_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='root_5', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_3', save_aps=False, save_steps=5000, seed=3, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/11/2021 09:09:43 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/11/2021 09:10:07 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/11/2021 09:10:10 - INFO - __main__ -   ***** Running training *****\n","08/11/2021 09:10:10 - INFO - __main__ -     Num examples = 164544\n","08/11/2021 09:10:10 - INFO - __main__ -     Num Epochs = 1\n","08/11/2021 09:10:10 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/11/2021 09:10:10 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/11/2021 09:10:10 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/11/2021 09:10:10 - INFO - __main__ -     Total optimization steps = 2571\n","08/11/2021 09:10:10 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/11/2021 09:10:10 - INFO - __main__ -     data_loaders = [('pacing_function_root_5', <torch.utils.data.dataloader.DataLoader object at 0x7f44b8b6e810>)]\n","08/11/2021 09:10:10 - INFO - __main__ -   Starting epoch 1\n","08/11/2021 09:10:10 - INFO - __main__ -   Training with pacing_function_root_5\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/11/2021 09:18:16 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/11/2021 09:18:16 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/11/2021 09:18:16 - INFO - __main__ -   loss = 0.3425005139907201\n","08/11/2021 09:18:16 - INFO - __main__ -   Current data iter size: 1717\n","08/11/2021 09:18:16 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 09:18:36 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 09:18:36 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 09:18:36 - INFO - __main__ -     Batch size = 64\n","08/11/2021 09:27:08 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 09:27:08 - INFO - __main__ -     map = 0.7472158052309658\n","08/11/2021 09:27:10 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_3\n","08/11/2021 09:35:19 - INFO - __main__ -   Iter = 600.0\n","08/11/2021 09:35:19 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/11/2021 09:35:19 - INFO - __main__ -   loss = 0.3320383485158285\n","08/11/2021 09:35:19 - INFO - __main__ -   Current data iter size: 1967\n","08/11/2021 09:35:19 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 09:35:37 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 09:35:37 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 09:35:37 - INFO - __main__ -     Batch size = 64\n","08/11/2021 09:44:08 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 09:44:08 - INFO - __main__ -     map = 0.7583574669923382\n","08/11/2021 09:44:10 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_3\n","08/11/2021 09:52:20 - INFO - __main__ -   Iter = 900.0\n","08/11/2021 09:52:20 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/11/2021 09:52:20 - INFO - __main__ -   loss = 0.3680109434823195\n","08/11/2021 09:52:20 - INFO - __main__ -   Current data iter size: 2131\n","08/11/2021 09:52:20 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 09:52:36 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 09:52:36 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 09:52:36 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:01:09 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:01:09 - INFO - __main__ -     map = 0.7548146289733934\n","08/11/2021 10:09:19 - INFO - __main__ -   Iter = 1200.0\n","08/11/2021 10:09:19 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/11/2021 10:09:19 - INFO - __main__ -   loss = 0.3954332778851191\n","08/11/2021 10:09:19 - INFO - __main__ -   Current data iter size: 2256\n","08/11/2021 10:09:19 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 10:09:36 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 10:09:36 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 10:09:36 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:18:07 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:18:07 - INFO - __main__ -     map = 0.7569639698171065\n","08/11/2021 10:26:17 - INFO - __main__ -   Iter = 1500.0\n","08/11/2021 10:26:17 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/11/2021 10:26:17 - INFO - __main__ -   loss = 0.4065923405687014\n","08/11/2021 10:26:17 - INFO - __main__ -   Current data iter size: 2359\n","08/11/2021 10:26:17 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 10:26:33 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 10:26:33 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 10:26:33 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:35:05 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:35:05 - INFO - __main__ -     map = 0.7656542209483198\n","08/11/2021 10:35:07 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_3\n","08/11/2021 10:43:17 - INFO - __main__ -   Iter = 1800.0\n","08/11/2021 10:43:17 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/11/2021 10:43:17 - INFO - __main__ -   loss = 0.4202004656692346\n","08/11/2021 10:43:17 - INFO - __main__ -   Current data iter size: 2446\n","08/11/2021 10:43:17 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 10:43:33 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 10:43:33 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 10:43:33 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:52:03 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:52:03 - INFO - __main__ -     map = 0.7673780018823635\n","08/11/2021 10:52:05 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_3\n","08/11/2021 11:00:15 - INFO - __main__ -   Iter = 2100.0\n","08/11/2021 11:00:15 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/11/2021 11:00:15 - INFO - __main__ -   loss = 0.443410397619009\n","08/11/2021 11:00:15 - INFO - __main__ -   Current data iter size: 2522\n","08/11/2021 11:00:15 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 11:00:32 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 11:00:32 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 11:00:32 - INFO - __main__ -     Batch size = 64\n","08/11/2021 11:09:03 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 11:09:03 - INFO - __main__ -     map = 0.7740103394669027\n","08/11/2021 11:09:05 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root5_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_5_seed_3\n","08/11/2021 11:17:16 - INFO - __main__ -   Iter = 2400.0\n","08/11/2021 11:17:16 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/11/2021 11:17:16 - INFO - __main__ -   loss = 0.4653773684302966\n","08/11/2021 11:17:16 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 11:17:16 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 11:17:34 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 11:17:34 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 11:17:34 - INFO - __main__ -     Batch size = 64\n","08/11/2021 11:26:07 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 11:26:07 - INFO - __main__ -     map = 0.7677319582590273\n","08/11/2021 11:30:48 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/11/2021 11:30:48 - INFO - __main__ -    global_step = 2572.0, average loss = 0.40134919253445894\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QMUN9JcQkgZ1"},"source":["# Mantis root-10"]},{"cell_type":"code","metadata":{"id":"vqk2nLyOIrbT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628531820326,"user_tz":-120,"elapsed":8013488,"user":{"displayName":"T. Z.","photoUrl":"","userId":"04670299702794629529"}},"outputId":"78567084-898c-49c9-bba7-f38887bcec40"},"source":["# seed_1 6w\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 1\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_root10_seed_1_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function root_10\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/09/2021 15:43:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/09/2021 15:43:32 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpwy3t4iwe\n","100% 433/433 [00:00<00:00, 435002.07B/s]\n","08/09/2021 15:43:32 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpwy3t4iwe to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 15:43:32 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 15:43:32 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpwy3t4iwe\n","08/09/2021 15:43:32 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 15:43:32 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/09/2021 15:43:33 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmparowci7e\n","100% 231508/231508 [00:00<00:00, 1243945.22B/s]\n","08/09/2021 15:43:33 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmparowci7e to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 15:43:33 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 15:43:33 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmparowci7e\n","08/09/2021 15:43:33 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 15:43:33 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpbns7n7qb\n","100% 440473133/440473133 [00:12<00:00, 34531448.67B/s]\n","08/09/2021 15:43:46 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpbns7n7qb to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 15:43:48 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 15:43:48 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpbns7n7qb\n","08/09/2021 15:43:48 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 15:43:51 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/09/2021 15:43:51 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/09/2021 15:44:03 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_root10_seed_1_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='root_10', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_1', save_aps=False, save_steps=5000, seed=1, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/09/2021 15:44:03 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/09/2021 15:44:24 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/09/2021 15:44:26 - INFO - __main__ -   ***** Running training *****\n","08/09/2021 15:44:26 - INFO - __main__ -     Num examples = 164544\n","08/09/2021 15:44:26 - INFO - __main__ -     Num Epochs = 1\n","08/09/2021 15:44:26 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/09/2021 15:44:26 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/09/2021 15:44:26 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/09/2021 15:44:26 - INFO - __main__ -     Total optimization steps = 2571\n","08/09/2021 15:44:26 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/09/2021 15:44:26 - INFO - __main__ -     data_loaders = [('pacing_function_root_10', <torch.utils.data.dataloader.DataLoader object at 0x7fd2f0838890>)]\n","08/09/2021 15:44:26 - INFO - __main__ -   Starting epoch 1\n","08/09/2021 15:44:26 - INFO - __main__ -   Training with pacing_function_root_10\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/09/2021 15:52:02 - INFO - __main__ -   Iter = 300\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/09/2021 15:52:02 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/09/2021 15:52:02 - INFO - __main__ -   loss = 0.44176608820756275\n","08/09/2021 15:52:02 - INFO - __main__ -   Current data iter size: 2096\n","08/09/2021 15:52:02 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:52:24 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:52:24 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:52:24 - INFO - __main__ -     Batch size = 64\n","08/09/2021 16:00:24 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 16:00:24 - INFO - __main__ -     map = 0.7392771544445907\n","08/09/2021 16:00:25 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root10_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_1\n","08/09/2021 16:08:08 - INFO - __main__ -   Iter = 600\n","08/09/2021 16:08:08 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/09/2021 16:08:08 - INFO - __main__ -   loss = 0.43850370933612187\n","08/09/2021 16:08:08 - INFO - __main__ -   Current data iter size: 2246\n","08/09/2021 16:08:08 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 16:08:25 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 16:08:25 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 16:08:25 - INFO - __main__ -     Batch size = 64\n","08/09/2021 16:16:23 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 16:16:23 - INFO - __main__ -     map = 0.7543313729575429\n","08/09/2021 16:16:25 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root10_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_1\n","08/09/2021 16:24:08 - INFO - __main__ -   Iter = 900\n","08/09/2021 16:24:08 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/09/2021 16:24:08 - INFO - __main__ -   loss = 0.44086192319790524\n","08/09/2021 16:24:08 - INFO - __main__ -   Current data iter size: 2340\n","08/09/2021 16:24:08 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 16:24:24 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 16:24:24 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 16:24:24 - INFO - __main__ -     Batch size = 64\n","08/09/2021 16:32:25 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 16:32:25 - INFO - __main__ -     map = 0.7601221561399983\n","08/09/2021 16:32:26 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root10_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_1\n","08/09/2021 16:40:10 - INFO - __main__ -   Iter = 1200\n","08/09/2021 16:40:10 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/09/2021 16:40:10 - INFO - __main__ -   loss = 0.44244666387637455\n","08/09/2021 16:40:10 - INFO - __main__ -   Current data iter size: 2408\n","08/09/2021 16:40:10 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 16:40:26 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 16:40:26 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 16:40:26 - INFO - __main__ -     Batch size = 64\n","08/09/2021 16:48:26 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 16:48:26 - INFO - __main__ -     map = 0.7619014170526147\n","08/09/2021 16:48:28 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root10_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_1\n","08/09/2021 16:56:12 - INFO - __main__ -   Iter = 1500\n","08/09/2021 16:56:12 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/09/2021 16:56:12 - INFO - __main__ -   loss = 0.4468220293521881\n","08/09/2021 16:56:12 - INFO - __main__ -   Current data iter size: 2462\n","08/09/2021 16:56:12 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 16:56:28 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 16:56:28 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 16:56:28 - INFO - __main__ -     Batch size = 64\n","08/09/2021 17:04:27 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 17:04:27 - INFO - __main__ -     map = 0.7551990951780168\n","08/09/2021 17:12:11 - INFO - __main__ -   Iter = 1800\n","08/09/2021 17:12:11 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/09/2021 17:12:11 - INFO - __main__ -   loss = 0.44095868716637293\n","08/09/2021 17:12:11 - INFO - __main__ -   Current data iter size: 2508\n","08/09/2021 17:12:11 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 17:12:27 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 17:12:27 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 17:12:27 - INFO - __main__ -     Batch size = 64\n","08/09/2021 17:20:28 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 17:20:28 - INFO - __main__ -     map = 0.7645747412450944\n","08/09/2021 17:20:30 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root10_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_1\n","08/09/2021 17:28:13 - INFO - __main__ -   Iter = 2100\n","08/09/2021 17:28:13 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/09/2021 17:28:13 - INFO - __main__ -   loss = 0.4503881666064262\n","08/09/2021 17:28:13 - INFO - __main__ -   Current data iter size: 2547\n","08/09/2021 17:28:13 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 17:28:30 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 17:28:30 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 17:28:30 - INFO - __main__ -     Batch size = 64\n","08/09/2021 17:36:30 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 17:36:30 - INFO - __main__ -     map = 0.7598685597404307\n","08/09/2021 17:44:13 - INFO - __main__ -   Iter = 2400\n","08/09/2021 17:44:13 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/09/2021 17:44:13 - INFO - __main__ -   loss = 0.4599758924047152\n","08/09/2021 17:44:13 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 17:44:13 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 17:44:30 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 17:44:30 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 17:44:30 - INFO - __main__ -     Batch size = 64\n","08/09/2021 17:52:30 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 17:52:30 - INFO - __main__ -     map = 0.7711285413267539\n","08/09/2021 17:52:32 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root10_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_1\n","08/09/2021 17:56:58 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/09/2021 17:56:58 - INFO - __main__ -    global_step = 2572, average loss = 0.4466546897662001\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2aJSTtWIgd2","executionInfo":{"status":"ok","timestamp":1628633388103,"user_tz":-120,"elapsed":7760668,"user":{"displayName":"xiaokang qian","photoUrl":"","userId":"16136989389156139825"}},"outputId":"a07a82f6-1697-48af-dc20-aa16c5c0bf06"},"source":["# seed_1\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 2\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_root_10_seed_2_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function root_10\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/10/2021 20:00:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/10/2021 20:00:34 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpkpymmvug\n","100% 433/433 [00:00<00:00, 398449.68B/s]\n","08/10/2021 20:00:34 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpkpymmvug to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:00:34 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:00:34 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpkpymmvug\n","08/10/2021 20:00:34 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:00:34 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/10/2021 20:00:34 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpb5n_wcyp\n","100% 231508/231508 [00:00<00:00, 924590.33B/s]\n","08/10/2021 20:00:35 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpb5n_wcyp to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:00:35 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:00:35 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpb5n_wcyp\n","08/10/2021 20:00:35 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:00:35 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp0c86dvvw\n","100% 440473133/440473133 [00:10<00:00, 40100324.79B/s]\n","08/10/2021 20:00:46 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp0c86dvvw to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:00:48 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:00:48 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp0c86dvvw\n","08/10/2021 20:00:48 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:00:51 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/10/2021 20:00:51 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/10/2021 20:01:03 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_root_10_seed_2_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='root_10', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_2', save_aps=False, save_steps=5000, seed=2, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/10/2021 20:01:03 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/10/2021 20:01:24 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/10/2021 20:01:27 - INFO - __main__ -   ***** Running training *****\n","08/10/2021 20:01:27 - INFO - __main__ -     Num examples = 164544\n","08/10/2021 20:01:27 - INFO - __main__ -     Num Epochs = 1\n","08/10/2021 20:01:27 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/10/2021 20:01:27 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/10/2021 20:01:27 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/10/2021 20:01:27 - INFO - __main__ -     Total optimization steps = 2571\n","08/10/2021 20:01:27 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/10/2021 20:01:27 - INFO - __main__ -     data_loaders = [('pacing_function_root_10', <torch.utils.data.dataloader.DataLoader object at 0x7fd582834890>)]\n","08/10/2021 20:01:27 - INFO - __main__ -   Starting epoch 1\n","08/10/2021 20:01:27 - INFO - __main__ -   Training with pacing_function_root_10\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/10/2021 20:08:54 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/10/2021 20:08:54 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/10/2021 20:08:54 - INFO - __main__ -   loss = 0.4634202418724696\n","08/10/2021 20:08:54 - INFO - __main__ -   Current data iter size: 2096\n","08/10/2021 20:08:54 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:09:14 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:09:14 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:09:14 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:16:56 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:16:56 - INFO - __main__ -     map = 0.7287275055481759\n","08/10/2021 20:16:58 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_2\n","08/10/2021 20:24:29 - INFO - __main__ -   Iter = 600.0\n","08/10/2021 20:24:29 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/10/2021 20:24:29 - INFO - __main__ -   loss = 0.42718275437752407\n","08/10/2021 20:24:29 - INFO - __main__ -   Current data iter size: 2246\n","08/10/2021 20:24:29 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:24:46 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:24:46 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:24:46 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:32:27 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:32:27 - INFO - __main__ -     map = 0.7402348744941106\n","08/10/2021 20:32:29 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_2\n","08/10/2021 20:40:00 - INFO - __main__ -   Iter = 900.0\n","08/10/2021 20:40:00 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/10/2021 20:40:00 - INFO - __main__ -   loss = 0.43774789323409397\n","08/10/2021 20:40:00 - INFO - __main__ -   Current data iter size: 2340\n","08/10/2021 20:40:00 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:40:16 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:40:16 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:40:16 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:47:57 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:47:57 - INFO - __main__ -     map = 0.74892624335822\n","08/10/2021 20:47:59 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_2\n","08/10/2021 20:55:30 - INFO - __main__ -   Iter = 1200.0\n","08/10/2021 20:55:30 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/10/2021 20:55:30 - INFO - __main__ -   loss = 0.44155491640170413\n","08/10/2021 20:55:30 - INFO - __main__ -   Current data iter size: 2408\n","08/10/2021 20:55:30 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:55:48 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:55:48 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:55:48 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:03:29 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:03:29 - INFO - __main__ -     map = 0.7547486567697244\n","08/10/2021 21:03:31 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_2\n","08/10/2021 21:11:02 - INFO - __main__ -   Iter = 1500.0\n","08/10/2021 21:11:02 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/10/2021 21:11:02 - INFO - __main__ -   loss = 0.44401163066426913\n","08/10/2021 21:11:02 - INFO - __main__ -   Current data iter size: 2462\n","08/10/2021 21:11:02 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:11:18 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:11:18 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:11:18 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:18:59 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:18:59 - INFO - __main__ -     map = 0.7511114770132191\n","08/10/2021 21:26:30 - INFO - __main__ -   Iter = 1800.0\n","08/10/2021 21:26:30 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/10/2021 21:26:30 - INFO - __main__ -   loss = 0.4548897576332092\n","08/10/2021 21:26:30 - INFO - __main__ -   Current data iter size: 2508\n","08/10/2021 21:26:30 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:26:46 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:26:46 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:26:46 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:34:27 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:34:27 - INFO - __main__ -     map = 0.7595151788682529\n","08/10/2021 21:34:28 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_2\n","08/10/2021 21:42:00 - INFO - __main__ -   Iter = 2100.0\n","08/10/2021 21:42:00 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/10/2021 21:42:00 - INFO - __main__ -   loss = 0.46088684648275374\n","08/10/2021 21:42:00 - INFO - __main__ -   Current data iter size: 2547\n","08/10/2021 21:42:00 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:42:16 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:42:16 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:42:16 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:49:55 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:49:55 - INFO - __main__ -     map = 0.7600223911479597\n","08/10/2021 21:49:57 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_2\n","08/10/2021 21:57:28 - INFO - __main__ -   Iter = 2400.0\n","08/10/2021 21:57:28 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/10/2021 21:57:28 - INFO - __main__ -   loss = 0.46657044897476835\n","08/10/2021 21:57:28 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 21:57:28 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:57:46 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:57:46 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:57:46 - INFO - __main__ -     Batch size = 64\n","08/10/2021 22:05:26 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 22:05:26 - INFO - __main__ -     map = 0.7676076983415095\n","08/10/2021 22:05:28 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_2\n","08/10/2021 22:09:46 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/10/2021 22:09:47 - INFO - __main__ -    global_step = 2572.0, average loss = 0.4509230869681709\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSIRr4KgAaFR","outputId":"a20643dc-1816-4170-87d3-81680b30568e"},"source":["# seed_3\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 3\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_root_10_seed_3_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function root_10\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/11/2021 11:38:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/11/2021 11:38:55 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp6ykc18aa\n","100% 433/433 [00:00<00:00, 409777.44B/s]\n","08/11/2021 11:38:55 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp6ykc18aa to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 11:38:55 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 11:38:55 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp6ykc18aa\n","08/11/2021 11:38:55 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 11:38:55 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/11/2021 11:38:55 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmphcrv61cp\n","100% 231508/231508 [00:00<00:00, 1254251.19B/s]\n","08/11/2021 11:38:56 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmphcrv61cp to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 11:38:56 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 11:38:56 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmphcrv61cp\n","08/11/2021 11:38:56 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 11:38:56 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpzhnwftgj\n","100% 440473133/440473133 [00:11<00:00, 39932871.00B/s]\n","08/11/2021 11:39:08 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpzhnwftgj to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 11:39:09 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 11:39:09 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpzhnwftgj\n","08/11/2021 11:39:09 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 11:39:12 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/11/2021 11:39:12 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/11/2021 11:39:24 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_root_10_seed_3_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='root_10', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_3', save_aps=False, save_steps=5000, seed=3, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/11/2021 11:39:24 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/11/2021 11:39:46 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/11/2021 11:39:50 - INFO - __main__ -   ***** Running training *****\n","08/11/2021 11:39:50 - INFO - __main__ -     Num examples = 164544\n","08/11/2021 11:39:50 - INFO - __main__ -     Num Epochs = 1\n","08/11/2021 11:39:50 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/11/2021 11:39:50 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/11/2021 11:39:50 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/11/2021 11:39:50 - INFO - __main__ -     Total optimization steps = 2571\n","08/11/2021 11:39:50 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/11/2021 11:39:50 - INFO - __main__ -     data_loaders = [('pacing_function_root_10', <torch.utils.data.dataloader.DataLoader object at 0x7fa2de207910>)]\n","08/11/2021 11:39:50 - INFO - __main__ -   Starting epoch 1\n","08/11/2021 11:39:50 - INFO - __main__ -   Training with pacing_function_root_10\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/11/2021 11:47:25 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/11/2021 11:47:25 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/11/2021 11:47:25 - INFO - __main__ -   loss = 0.45842121620972953\n","08/11/2021 11:47:25 - INFO - __main__ -   Current data iter size: 2096\n","08/11/2021 11:47:25 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 11:47:46 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 11:47:46 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 11:47:46 - INFO - __main__ -     Batch size = 64\n","08/11/2021 11:55:53 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 11:55:53 - INFO - __main__ -     map = 0.7338544081364712\n","08/11/2021 11:55:54 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_3\n","08/11/2021 12:03:44 - INFO - __main__ -   Iter = 600.0\n","08/11/2021 12:03:44 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/11/2021 12:03:44 - INFO - __main__ -   loss = 0.4327078391114871\n","08/11/2021 12:03:44 - INFO - __main__ -   Current data iter size: 2246\n","08/11/2021 12:03:44 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 12:04:01 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 12:04:01 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 12:04:01 - INFO - __main__ -     Batch size = 64\n","08/11/2021 12:12:08 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 12:12:08 - INFO - __main__ -     map = 0.75314313131052\n","08/11/2021 12:12:10 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_3\n","08/11/2021 12:20:00 - INFO - __main__ -   Iter = 900.0\n","08/11/2021 12:20:00 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/11/2021 12:20:00 - INFO - __main__ -   loss = 0.42553069521983466\n","08/11/2021 12:20:00 - INFO - __main__ -   Current data iter size: 2340\n","08/11/2021 12:20:00 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 12:20:18 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 12:20:18 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 12:20:18 - INFO - __main__ -     Batch size = 64\n","08/11/2021 12:28:24 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 12:28:24 - INFO - __main__ -     map = 0.7558521456718796\n","08/11/2021 12:28:25 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_3\n","08/11/2021 12:36:15 - INFO - __main__ -   Iter = 1200.0\n","08/11/2021 12:36:15 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/11/2021 12:36:15 - INFO - __main__ -   loss = 0.43750825136899946\n","08/11/2021 12:36:15 - INFO - __main__ -   Current data iter size: 2408\n","08/11/2021 12:36:15 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 12:36:33 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 12:36:33 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 12:36:33 - INFO - __main__ -     Batch size = 64\n","08/11/2021 12:44:40 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 12:44:40 - INFO - __main__ -     map = 0.7610522067691692\n","08/11/2021 12:44:41 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_3\n","08/11/2021 12:52:31 - INFO - __main__ -   Iter = 1500.0\n","08/11/2021 12:52:31 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/11/2021 12:52:31 - INFO - __main__ -   loss = 0.4432945158084234\n","08/11/2021 12:52:31 - INFO - __main__ -   Current data iter size: 2462\n","08/11/2021 12:52:31 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 12:52:47 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 12:52:47 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 12:52:47 - INFO - __main__ -     Batch size = 64\n","08/11/2021 13:00:54 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 13:00:54 - INFO - __main__ -     map = 0.7630435409289573\n","08/11/2021 13:00:56 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_root_10_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3root_10_seed_3\n","08/11/2021 13:08:45 - INFO - __main__ -   Iter = 1800.0\n","08/11/2021 13:08:45 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/11/2021 13:08:45 - INFO - __main__ -   loss = 0.4420505998531977\n","08/11/2021 13:08:45 - INFO - __main__ -   Current data iter size: 2508\n","08/11/2021 13:08:45 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 13:09:02 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 13:09:02 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 13:09:02 - INFO - __main__ -     Batch size = 64\n","08/11/2021 13:17:06 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 13:17:06 - INFO - __main__ -     map = 0.7618895799434162\n","08/11/2021 13:24:56 - INFO - __main__ -   Iter = 2100.0\n","08/11/2021 13:24:56 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/11/2021 13:24:56 - INFO - __main__ -   loss = 0.45395415132244427\n","08/11/2021 13:24:56 - INFO - __main__ -   Current data iter size: 2547\n","08/11/2021 13:24:56 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 13:25:14 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 13:25:14 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 13:25:14 - INFO - __main__ -     Batch size = 64\n"],"name":"stdout"}]}]}