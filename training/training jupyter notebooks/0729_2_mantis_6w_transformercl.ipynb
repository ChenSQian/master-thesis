{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0729_2_transformercl_reproduce.ipynb","provenance":[{"file_id":"1MR6fqyFJ0F0CvZ_prOyG8prQRr9h1zwL","timestamp":1621518416179}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ne7AwwgCqH2P"},"source":["function KeepClicking(){\n","console.log(\"Clicking\");\n","document.querySelector(\"colab-connect-button\").click()\n","}\n","setInterval(KeepClicking,60000)"]},{"cell_type":"code","metadata":{"id":"1QWVTurKCTMV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628682024044,"user_tz":-120,"elapsed":42642,"user":{"displayName":"王梦琪","photoUrl":"","userId":"17110178610712270742"}},"outputId":"c4d92485-0670-4b8c-8afb-940f802e9b83"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0q-7RWqYrMbI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628682037266,"user_tz":-120,"elapsed":10966,"user":{"displayName":"王梦琪","photoUrl":"","userId":"17110178610712270742"}},"outputId":"d3c0a148-6b19-481c-aa9b-5b3fc800e3fb"},"source":["# install transformers\n","!pip install pytorch_transformers\n","# Mount google drive\n","!pip install -r drive/MyDrive/transformers_cl/requirements.txt"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pytorch_transformers\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 176 kB 9.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (4.41.1)\n","Collecting boto3\n","  Downloading boto3-1.18.18-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 49.1 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 64.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.19.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 59.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2.23.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.7.4.3)\n","Collecting botocore<1.22.0,>=1.21.18\n","  Downloading botocore-1.21.18-py3-none-any.whl (7.8 MB)\n","\u001b[K     |████████████████████████████████| 7.8 MB 53.5 MB/s \n","\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.18->boto3->pytorch_transformers) (2.8.1)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 75.8 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.18->boto3->pytorch_transformers) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2.10)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 71.7 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2021.5.30)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.18.18 botocore-1.21.18 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.0 sacremoses-0.0.45 sentencepiece-0.1.96 urllib3-1.25.11\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n","\u001b[K     |████████████████████████████████| 124 kB 8.3 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/transformers_cl/requirements.txt (line 2)) (0.22.2.post1)\n","Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (5.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r drive/MyDrive/transformers_cl/requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r drive/MyDrive/transformers_cl/requirements.txt (line 1)) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->-r drive/MyDrive/transformers_cl/requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r drive/MyDrive/transformers_cl/requirements.txt (line 2)) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r drive/MyDrive/transformers_cl/requirements.txt (line 2)) (1.0.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (57.2.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (5.0.5)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (1.0.18)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (4.4.2)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (2.6.1)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (0.8.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (0.7.5)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (0.2.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (0.2.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->-r drive/MyDrive/transformers_cl/requirements.txt (line 3)) (0.7.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DjrdQyKjnSjY"},"source":["# Mantis standard_training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKeKq2Rx7ri5","executionInfo":{"status":"ok","timestamp":1628592023157,"user_tz":-120,"elapsed":6012427,"user":{"displayName":"王梦琪","photoUrl":"","userId":"17110178610712270742"}},"outputId":"8f0d9ba1-b648-45a1-fa36-bc9a88d7067f"},"source":["# whole I added resume training but dont understand why it create features again, because the\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_whole/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1 \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 1\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_whole \\\n","    --logging_steps 500 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function standard_training\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --resume_epoch 2\\\n","    --tokenizer_name bert-base-uncased\\\n","    --cached_features_file_dir drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/10/2021 09:00:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/10/2021 09:00:11 - INFO - pytorch_transformers.modeling_utils -   loading configuration file drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_whole/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1/config.json\n","08/10/2021 09:00:11 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/10/2021 09:00:11 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 09:00:11 - INFO - pytorch_transformers.modeling_utils -   loading weights file drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_whole/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1/pytorch_model.bin\n","08/10/2021 09:00:18 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=-1, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_whole/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_whole', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='standard_training', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=2.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1', save_aps=False, save_steps=5000, seed=1, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='bert-base-uncased', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/10/2021 09:00:18 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/10/2021 09:00:36 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/10/2021 09:00:38 - INFO - __main__ -   ***** Running training *****\n","08/10/2021 09:00:38 - INFO - __main__ -     Num examples = 164544\n","08/10/2021 09:00:38 - INFO - __main__ -     Num Epochs = 1\n","08/10/2021 09:00:38 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/10/2021 09:00:38 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/10/2021 09:00:38 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/10/2021 09:00:38 - INFO - __main__ -     Total optimization steps = 2571\n","08/10/2021 09:00:38 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/10/2021 09:00:38 - INFO - __main__ -     data_loaders = [('pacing_function_standard_training', <torch.utils.data.dataloader.DataLoader object at 0x7f5df50c24d0>)]\n","08/10/2021 09:00:38 - INFO - __main__ -   Starting epoch 1\n","08/10/2021 09:00:38 - INFO - __main__ -   Training with pacing_function_standard_training\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/10/2021 09:11:56 - INFO - __main__ -   Iter = 3000.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/10/2021 09:11:56 - INFO - __main__ -   lr = 1.6662777129521586e-05\n","08/10/2021 09:11:56 - INFO - __main__ -   loss = 0.30846267876029015\n","08/10/2021 09:11:56 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 09:11:56 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 09:12:24 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 09:12:24 - INFO - __main__ -     Num examples = 180960\n","08/10/2021 09:12:24 - INFO - __main__ -     Batch size = 64\n","08/10/2021 09:37:05 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 09:37:05 - INFO - __main__ -     map = 0.7015498031661973\n","08/10/2021 09:37:07 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_whole/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1\n","08/10/2021 09:50:16 - INFO - __main__ -   Iter = 3500.0\n","08/10/2021 09:50:16 - INFO - __main__ -   lr = 1.2773239984441852e-05\n","08/10/2021 09:50:16 - INFO - __main__ -   loss = 0.31471646624803545\n","08/10/2021 09:50:16 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 09:50:16 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 09:50:38 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 09:50:38 - INFO - __main__ -     Num examples = 180960\n","08/10/2021 09:50:38 - INFO - __main__ -     Batch size = 64\n","08/10/2021 10:15:17 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 10:15:17 - INFO - __main__ -     map = 0.7107536480920268\n","08/10/2021 10:15:19 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_whole/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1\n","08/10/2021 10:28:28 - INFO - __main__ -   Iter = 4000.0\n","08/10/2021 10:28:28 - INFO - __main__ -   lr = 8.883702839362117e-06\n","08/10/2021 10:28:28 - INFO - __main__ -   loss = 0.33455853225290777\n","08/10/2021 10:28:28 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 10:28:28 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 10:28:49 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 10:28:49 - INFO - __main__ -     Num examples = 180960\n","08/10/2021 10:28:49 - INFO - __main__ -     Batch size = 64\n","Traceback (most recent call last):\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 712, in <module>\n","    main()\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 659, in main\n","    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 245, in train\n","    results = evaluate(args, model, tokenizer)\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 361, in evaluate\n","    all_losses.append(tmp_eval_loss.mean().item())\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kAaRXJmYI8Ij","executionInfo":{"status":"ok","timestamp":1628524541216,"user_tz":-120,"elapsed":8104680,"user":{"displayName":"王梦琪","photoUrl":"","userId":"17110178610712270742"}},"outputId":"4e963263-500d-4a6d-b902-a25d0df7eef8"},"source":["# 6w\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 1\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function standard_training\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/09/2021 13:40:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/09/2021 13:40:42 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp4k13ubwu\n","100% 433/433 [00:00<00:00, 523381.45B/s]\n","08/09/2021 13:40:42 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp4k13ubwu to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 13:40:42 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 13:40:42 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp4k13ubwu\n","08/09/2021 13:40:42 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 13:40:42 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/09/2021 13:40:42 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpf73w8390\n","100% 231508/231508 [00:00<00:00, 943803.20B/s]\n","08/09/2021 13:40:43 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpf73w8390 to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 13:40:43 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 13:40:43 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpf73w8390\n","08/09/2021 13:40:43 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 13:40:43 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpx_2eoa7r\n","100% 440473133/440473133 [00:12<00:00, 35365002.98B/s]\n","08/09/2021 13:40:56 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpx_2eoa7r to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 13:40:57 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 13:40:57 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpx_2eoa7r\n","08/09/2021 13:40:57 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 13:41:00 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/09/2021 13:41:00 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/09/2021 13:41:12 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='standard_training', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1', save_aps=False, save_steps=5000, seed=1, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/09/2021 13:41:12 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/09/2021 13:41:33 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/09/2021 13:41:36 - INFO - __main__ -   ***** Running training *****\n","08/09/2021 13:41:36 - INFO - __main__ -     Num examples = 164544\n","08/09/2021 13:41:36 - INFO - __main__ -     Num Epochs = 1\n","08/09/2021 13:41:36 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/09/2021 13:41:36 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/09/2021 13:41:36 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/09/2021 13:41:36 - INFO - __main__ -     Total optimization steps = 2571\n","08/09/2021 13:41:36 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/09/2021 13:41:36 - INFO - __main__ -     data_loaders = [('pacing_function_standard_training', <torch.utils.data.dataloader.DataLoader object at 0x7f5ba0188550>)]\n","08/09/2021 13:41:36 - INFO - __main__ -   Starting epoch 1\n","08/09/2021 13:41:36 - INFO - __main__ -   Training with pacing_function_standard_training\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/09/2021 13:49:22 - INFO - __main__ -   Iter = 300\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/09/2021 13:49:22 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/09/2021 13:49:22 - INFO - __main__ -   loss = 0.563107757071654\n","08/09/2021 13:49:22 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 13:49:22 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 13:49:43 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 13:49:43 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 13:49:43 - INFO - __main__ -     Batch size = 64\n","08/09/2021 13:57:55 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 13:57:55 - INFO - __main__ -     map = 0.741010746838909\n","08/09/2021 13:57:57 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1\n","08/09/2021 14:05:50 - INFO - __main__ -   Iter = 600\n","08/09/2021 14:05:50 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/09/2021 14:05:50 - INFO - __main__ -   loss = 0.5220635311802229\n","08/09/2021 14:05:50 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 14:05:50 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:06:06 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:06:06 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:06:06 - INFO - __main__ -     Batch size = 64\n","08/09/2021 14:14:17 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 14:14:17 - INFO - __main__ -     map = 0.7545654316075107\n","08/09/2021 14:14:19 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1\n","08/09/2021 14:22:11 - INFO - __main__ -   Iter = 900\n","08/09/2021 14:22:11 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/09/2021 14:22:11 - INFO - __main__ -   loss = 0.5054762207468351\n","08/09/2021 14:22:11 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 14:22:11 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:22:28 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:22:28 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:22:28 - INFO - __main__ -     Batch size = 64\n","08/09/2021 14:30:38 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 14:30:38 - INFO - __main__ -     map = 0.7567839473942348\n","08/09/2021 14:30:40 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1\n","08/09/2021 14:38:34 - INFO - __main__ -   Iter = 1200\n","08/09/2021 14:38:34 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/09/2021 14:38:34 - INFO - __main__ -   loss = 0.4973410240809123\n","08/09/2021 14:38:34 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 14:38:34 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:38:52 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:38:52 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:38:52 - INFO - __main__ -     Batch size = 64\n","08/09/2021 14:47:03 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 14:47:03 - INFO - __main__ -     map = 0.7610691305776691\n","08/09/2021 14:47:05 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1\n","08/09/2021 14:54:55 - INFO - __main__ -   Iter = 1500\n","08/09/2021 14:54:55 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/09/2021 14:54:55 - INFO - __main__ -   loss = 0.48165073841810224\n","08/09/2021 14:54:55 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 14:54:55 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:55:11 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:55:11 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:55:11 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:03:02 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:03:02 - INFO - __main__ -     map = 0.7600578044378431\n","08/09/2021 15:10:42 - INFO - __main__ -   Iter = 1800\n","08/09/2021 15:10:42 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/09/2021 15:10:42 - INFO - __main__ -   loss = 0.4766615119576454\n","08/09/2021 15:10:42 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 15:10:42 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:10:58 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:10:58 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:10:58 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:18:50 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:18:50 - INFO - __main__ -     map = 0.7674816753964852\n","08/09/2021 15:18:52 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1\n","08/09/2021 15:26:31 - INFO - __main__ -   Iter = 2100\n","08/09/2021 15:26:31 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/09/2021 15:26:31 - INFO - __main__ -   loss = 0.4662831456462542\n","08/09/2021 15:26:31 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 15:26:31 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:26:48 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:26:48 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:26:48 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:34:45 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:34:45 - INFO - __main__ -     map = 0.7637394495086002\n","08/09/2021 15:42:38 - INFO - __main__ -   Iter = 2400\n","08/09/2021 15:42:38 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/09/2021 15:42:38 - INFO - __main__ -   loss = 0.45593655000130334\n","08/09/2021 15:42:38 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 15:42:38 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:42:55 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:42:55 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:42:55 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:51:05 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:51:05 - INFO - __main__ -     map = 0.7682885289175231\n","08/09/2021 15:51:07 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_1\n","08/09/2021 15:55:39 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/09/2021 15:55:39 - INFO - __main__ -    global_step = 2572, average loss = 0.49407059754915594\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_JWOyaXgM5L","executionInfo":{"status":"ok","timestamp":1628613751783,"user_tz":-120,"elapsed":7622924,"user":{"displayName":"王梦琪","photoUrl":"","userId":"17110178610712270742"}},"outputId":"cdd59e68-fcda-4ea0-ded5-af3d76fc1db8"},"source":["# seed 1 this one I can run another epoch, so I don't need to\n","# discard this result and restart running 2 epochs because this is standard training!\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 2\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_2_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function standard_training\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/10/2021 14:35:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/10/2021 14:35:34 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpbvgaxgyj\n","100% 433/433 [00:00<00:00, 397577.42B/s]\n","08/10/2021 14:35:34 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpbvgaxgyj to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 14:35:34 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 14:35:34 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpbvgaxgyj\n","08/10/2021 14:35:34 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 14:35:34 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/10/2021 14:35:34 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp5gf9f5pi\n","100% 231508/231508 [00:00<00:00, 943632.60B/s]\n","08/10/2021 14:35:35 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp5gf9f5pi to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 14:35:35 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 14:35:35 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp5gf9f5pi\n","08/10/2021 14:35:35 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 14:35:35 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpxjcha8id\n","100% 440473133/440473133 [00:10<00:00, 41831692.82B/s]\n","08/10/2021 14:35:46 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpxjcha8id to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 14:35:47 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 14:35:47 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpxjcha8id\n","08/10/2021 14:35:47 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 14:35:50 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/10/2021 14:35:50 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/10/2021 14:36:02 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_2_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='standard_training', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_2', save_aps=False, save_steps=5000, seed=2, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/10/2021 14:36:02 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/10/2021 14:36:23 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/10/2021 14:36:25 - INFO - __main__ -   ***** Running training *****\n","08/10/2021 14:36:25 - INFO - __main__ -     Num examples = 164544\n","08/10/2021 14:36:25 - INFO - __main__ -     Num Epochs = 1\n","08/10/2021 14:36:25 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/10/2021 14:36:25 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/10/2021 14:36:25 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/10/2021 14:36:25 - INFO - __main__ -     Total optimization steps = 2571\n","08/10/2021 14:36:25 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/10/2021 14:36:25 - INFO - __main__ -     data_loaders = [('pacing_function_standard_training', <torch.utils.data.dataloader.DataLoader object at 0x7fb3e3bb2a50>)]\n","08/10/2021 14:36:25 - INFO - __main__ -   Starting epoch 1\n","08/10/2021 14:36:25 - INFO - __main__ -   Training with pacing_function_standard_training\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/10/2021 14:43:37 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/10/2021 14:43:37 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/10/2021 14:43:37 - INFO - __main__ -   loss = 0.5846084194382032\n","08/10/2021 14:43:37 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 14:43:37 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 14:43:57 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 14:43:57 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 14:43:57 - INFO - __main__ -     Batch size = 64\n","08/10/2021 14:51:35 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 14:51:35 - INFO - __main__ -     map = 0.7160880865325379\n","08/10/2021 14:51:36 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_2\n","08/10/2021 14:58:55 - INFO - __main__ -   Iter = 600.0\n","08/10/2021 14:58:55 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/10/2021 14:58:55 - INFO - __main__ -   loss = 0.5248107563455899\n","08/10/2021 14:58:55 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 14:58:55 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 14:59:12 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 14:59:12 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 14:59:12 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:06:49 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:06:49 - INFO - __main__ -     map = 0.7313561366821308\n","08/10/2021 15:06:51 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_2\n","08/10/2021 15:14:10 - INFO - __main__ -   Iter = 900.0\n","08/10/2021 15:14:10 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/10/2021 15:14:10 - INFO - __main__ -   loss = 0.5092020241419474\n","08/10/2021 15:14:10 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 15:14:10 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 15:14:26 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 15:14:26 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 15:14:26 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:22:03 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:22:03 - INFO - __main__ -     map = 0.7503999921046901\n","08/10/2021 15:22:05 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_2\n","08/10/2021 15:29:24 - INFO - __main__ -   Iter = 1200.0\n","08/10/2021 15:29:24 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/10/2021 15:29:24 - INFO - __main__ -   loss = 0.5026592581470808\n","08/10/2021 15:29:24 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 15:29:24 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 15:29:42 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 15:29:42 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 15:29:42 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:37:19 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:37:19 - INFO - __main__ -     map = 0.7562339831336446\n","08/10/2021 15:37:21 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_2\n","08/10/2021 15:44:40 - INFO - __main__ -   Iter = 1500.0\n","08/10/2021 15:44:40 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/10/2021 15:44:40 - INFO - __main__ -   loss = 0.4949063292145729\n","08/10/2021 15:44:40 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 15:44:40 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 15:44:56 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 15:44:56 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 15:44:56 - INFO - __main__ -     Batch size = 64\n","08/10/2021 15:52:33 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 15:52:33 - INFO - __main__ -     map = 0.7540870916970234\n","08/10/2021 15:59:52 - INFO - __main__ -   Iter = 1800.0\n","08/10/2021 15:59:52 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/10/2021 15:59:52 - INFO - __main__ -   loss = 0.4805574447909991\n","08/10/2021 15:59:52 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 15:59:52 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 16:00:08 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 16:00:08 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 16:00:08 - INFO - __main__ -     Batch size = 64\n","08/10/2021 16:07:45 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 16:07:45 - INFO - __main__ -     map = 0.7589165663985333\n","08/10/2021 16:07:47 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_2\n","08/10/2021 16:15:06 - INFO - __main__ -   Iter = 2100.0\n","08/10/2021 16:15:06 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/10/2021 16:15:06 - INFO - __main__ -   loss = 0.4711037883162498\n","08/10/2021 16:15:06 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 16:15:06 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 16:15:23 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 16:15:23 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 16:15:23 - INFO - __main__ -     Batch size = 64\n","08/10/2021 16:23:00 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 16:23:00 - INFO - __main__ -     map = 0.7590222837022914\n","08/10/2021 16:23:02 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_2\n","08/10/2021 16:30:21 - INFO - __main__ -   Iter = 2400.0\n","08/10/2021 16:30:21 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/10/2021 16:30:21 - INFO - __main__ -   loss = 0.468821277320385\n","08/10/2021 16:30:21 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 16:30:21 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 16:30:38 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 16:30:38 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 16:30:38 - INFO - __main__ -     Batch size = 64\n","08/10/2021 16:38:15 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 16:38:15 - INFO - __main__ -     map = 0.7648070466323896\n","08/10/2021 16:38:17 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_2\n","08/10/2021 16:42:29 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/10/2021 16:42:29 - INFO - __main__ -    global_step = 2572.0, average loss = 0.5022689647636555\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V9v3HqRC_-W3","executionInfo":{"status":"ok","timestamp":1628681484450,"user_tz":-120,"elapsed":8314805,"user":{"displayName":"王梦琪","photoUrl":"","userId":"17110178610712270742"}},"outputId":"31faa305-98ab-4e3e-9358-ddd4eb6dde16"},"source":["# seed 1 this one I can run another epoch, so I don't need to\n","# discard this result and restart running 2 epochs because this is standard training!\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 3\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_3_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function standard_training\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/11/2021 09:12:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/11/2021 09:12:55 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpqnuue2vz\n","100% 433/433 [00:00<00:00, 472705.27B/s]\n","08/11/2021 09:12:55 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpqnuue2vz to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 09:12:55 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 09:12:55 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpqnuue2vz\n","08/11/2021 09:12:55 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 09:12:55 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/11/2021 09:12:55 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp8o5m7t_b\n","100% 231508/231508 [00:00<00:00, 839438.05B/s]\n","08/11/2021 09:12:56 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp8o5m7t_b to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 09:12:56 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 09:12:56 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp8o5m7t_b\n","08/11/2021 09:12:56 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 09:12:56 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpzc48g67c\n","100% 440473133/440473133 [00:13<00:00, 32986947.38B/s]\n","08/11/2021 09:13:10 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpzc48g67c to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 09:13:11 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 09:13:11 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpzc48g67c\n","08/11/2021 09:13:12 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 09:13:14 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/11/2021 09:13:14 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/11/2021 09:13:26 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_3_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='standard_training', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_3', save_aps=False, save_steps=5000, seed=3, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/11/2021 09:13:26 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/11/2021 09:13:47 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/11/2021 09:13:50 - INFO - __main__ -   ***** Running training *****\n","08/11/2021 09:13:50 - INFO - __main__ -     Num examples = 164544\n","08/11/2021 09:13:50 - INFO - __main__ -     Num Epochs = 1\n","08/11/2021 09:13:50 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/11/2021 09:13:50 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/11/2021 09:13:50 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/11/2021 09:13:50 - INFO - __main__ -     Total optimization steps = 2571\n","08/11/2021 09:13:50 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/11/2021 09:13:50 - INFO - __main__ -     data_loaders = [('pacing_function_standard_training', <torch.utils.data.dataloader.DataLoader object at 0x7f691127d850>)]\n","08/11/2021 09:13:50 - INFO - __main__ -   Starting epoch 1\n","08/11/2021 09:13:50 - INFO - __main__ -   Training with pacing_function_standard_training\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/11/2021 09:21:34 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/11/2021 09:21:34 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/11/2021 09:21:34 - INFO - __main__ -   loss = 0.5701240702470144\n","08/11/2021 09:21:34 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 09:21:34 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 09:21:55 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 09:21:55 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 09:21:55 - INFO - __main__ -     Batch size = 64\n","08/11/2021 09:30:13 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 09:30:13 - INFO - __main__ -     map = 0.7394289599789319\n","08/11/2021 09:30:15 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_3\n","08/11/2021 09:38:16 - INFO - __main__ -   Iter = 600.0\n","08/11/2021 09:38:16 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/11/2021 09:38:16 - INFO - __main__ -   loss = 0.524258807003498\n","08/11/2021 09:38:16 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 09:38:16 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 09:38:32 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 09:38:32 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 09:38:32 - INFO - __main__ -     Batch size = 64\n","08/11/2021 09:46:53 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 09:46:53 - INFO - __main__ -     map = 0.7482282670387389\n","08/11/2021 09:46:54 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_3\n","08/11/2021 09:54:56 - INFO - __main__ -   Iter = 900.0\n","08/11/2021 09:54:56 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/11/2021 09:54:56 - INFO - __main__ -   loss = 0.5083212072650591\n","08/11/2021 09:54:56 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 09:54:56 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 09:55:13 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 09:55:13 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 09:55:13 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:03:31 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:03:31 - INFO - __main__ -     map = 0.7487576570141601\n","08/11/2021 10:03:33 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_3\n","08/11/2021 10:11:34 - INFO - __main__ -   Iter = 1200.0\n","08/11/2021 10:11:34 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/11/2021 10:11:34 - INFO - __main__ -   loss = 0.4940532427032789\n","08/11/2021 10:11:34 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 10:11:34 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 10:11:50 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 10:11:50 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 10:11:50 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:20:08 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:20:08 - INFO - __main__ -     map = 0.7650043573081444\n","08/11/2021 10:20:10 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_3\n","08/11/2021 10:28:12 - INFO - __main__ -   Iter = 1500.0\n","08/11/2021 10:28:12 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/11/2021 10:28:12 - INFO - __main__ -   loss = 0.4846134093403816\n","08/11/2021 10:28:12 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 10:28:12 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 10:28:28 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 10:28:28 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 10:28:28 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:36:49 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:36:49 - INFO - __main__ -     map = 0.7616834112645642\n","08/11/2021 10:44:50 - INFO - __main__ -   Iter = 1800.0\n","08/11/2021 10:44:50 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/11/2021 10:44:50 - INFO - __main__ -   loss = 0.477952510813872\n","08/11/2021 10:44:50 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 10:44:50 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 10:45:07 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 10:45:07 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 10:45:07 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:53:27 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:53:27 - INFO - __main__ -     map = 0.7648876272051482\n","08/11/2021 11:01:28 - INFO - __main__ -   Iter = 2100.0\n","08/11/2021 11:01:28 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/11/2021 11:01:28 - INFO - __main__ -   loss = 0.46987965017557143\n","08/11/2021 11:01:28 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 11:01:28 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 11:01:45 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 11:01:45 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 11:01:45 - INFO - __main__ -     Batch size = 64\n","08/11/2021 11:10:04 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 11:10:04 - INFO - __main__ -     map = 0.7710563129946801\n","08/11/2021 11:10:06 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_standard_training_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3standard_training_seed_3\n","08/11/2021 11:18:08 - INFO - __main__ -   Iter = 2400.0\n","08/11/2021 11:18:08 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/11/2021 11:18:08 - INFO - __main__ -   loss = 0.46787169893582664\n","08/11/2021 11:18:08 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 11:18:08 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 11:18:25 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 11:18:25 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 11:18:25 - INFO - __main__ -     Batch size = 64\n","08/11/2021 11:26:46 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 11:26:46 - INFO - __main__ -     map = 0.7656117468511817\n","08/11/2021 11:31:22 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/11/2021 11:31:22 - INFO - __main__ -    global_step = 2572.0, average loss = 0.49718902386365743\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GDre8JHYn2VP"},"source":["# Mantis geom_progression"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w-FxGmq-JRK4","executionInfo":{"status":"ok","timestamp":1628529585760,"user_tz":-120,"elapsed":7739414,"user":{"displayName":"长安故里","photoUrl":"","userId":"04762263270908229891"}},"outputId":"fc2c24ba-011c-4d34-c7ef-4ff95a0ce428"},"source":["# seed 1 6w\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 1\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_1_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function geom_progression\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/09/2021 15:10:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/09/2021 15:10:51 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpzz1ay12d\n","100% 433/433 [00:00<00:00, 530257.99B/s]\n","08/09/2021 15:10:52 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpzz1ay12d to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 15:10:52 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 15:10:52 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpzz1ay12d\n","08/09/2021 15:10:52 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 15:10:52 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/09/2021 15:10:52 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpe9x4mjf1\n","100% 231508/231508 [00:00<00:00, 949657.14B/s]\n","08/09/2021 15:10:52 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpe9x4mjf1 to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 15:10:52 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 15:10:52 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpe9x4mjf1\n","08/09/2021 15:10:52 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 15:10:53 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpe1y_gnkb\n","100% 440473133/440473133 [00:12<00:00, 34820411.23B/s]\n","08/09/2021 15:11:06 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpe1y_gnkb to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 15:11:07 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 15:11:07 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpe1y_gnkb\n","08/09/2021 15:11:07 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 15:11:10 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/09/2021 15:11:10 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/09/2021 15:11:22 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_1_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='geom_progression', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_1', save_aps=False, save_steps=5000, seed=1, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/09/2021 15:11:22 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/09/2021 15:11:44 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/09/2021 15:11:47 - INFO - __main__ -   ***** Running training *****\n","08/09/2021 15:11:47 - INFO - __main__ -     Num examples = 164544\n","08/09/2021 15:11:47 - INFO - __main__ -     Num Epochs = 1\n","08/09/2021 15:11:47 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/09/2021 15:11:47 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/09/2021 15:11:47 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/09/2021 15:11:47 - INFO - __main__ -     Total optimization steps = 2571\n","08/09/2021 15:11:47 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/09/2021 15:11:47 - INFO - __main__ -     data_loaders = [('pacing_function_geom_progression', <torch.utils.data.dataloader.DataLoader object at 0x7f387c6cd950>)]\n","08/09/2021 15:11:47 - INFO - __main__ -   Starting epoch 1\n","08/09/2021 15:11:47 - INFO - __main__ -   Training with pacing_function_geom_progression\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/09/2021 15:19:11 - INFO - __main__ -   Iter = 300\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/09/2021 15:19:11 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/09/2021 15:19:11 - INFO - __main__ -   loss = 0.17428223146746555\n","08/09/2021 15:19:11 - INFO - __main__ -   Current data iter size: 980\n","08/09/2021 15:19:11 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:19:33 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:19:33 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:19:33 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:27:14 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:27:14 - INFO - __main__ -     map = 0.713831382841273\n","08/09/2021 15:27:16 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_1\n","08/09/2021 15:34:44 - INFO - __main__ -   Iter = 600\n","08/09/2021 15:34:44 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/09/2021 15:34:44 - INFO - __main__ -   loss = 0.11219948748437067\n","08/09/2021 15:34:44 - INFO - __main__ -   Current data iter size: 1131\n","08/09/2021 15:34:44 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:35:00 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:35:00 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:35:00 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:42:40 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:42:40 - INFO - __main__ -     map = 0.744080172158383\n","08/09/2021 15:42:41 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_1\n","08/09/2021 15:50:09 - INFO - __main__ -   Iter = 900\n","08/09/2021 15:50:09 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/09/2021 15:50:09 - INFO - __main__ -   loss = 0.12311243511115512\n","08/09/2021 15:50:09 - INFO - __main__ -   Current data iter size: 1306\n","08/09/2021 15:50:09 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:50:25 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:50:25 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:50:25 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:58:05 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:58:05 - INFO - __main__ -     map = 0.7445183668269485\n","08/09/2021 15:58:07 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_1\n","08/09/2021 16:05:35 - INFO - __main__ -   Iter = 1200\n","08/09/2021 16:05:35 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/09/2021 16:05:35 - INFO - __main__ -   loss = 0.14734629426772397\n","08/09/2021 16:05:35 - INFO - __main__ -   Current data iter size: 1507\n","08/09/2021 16:05:35 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 16:05:52 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 16:05:52 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 16:05:52 - INFO - __main__ -     Batch size = 64\n","08/09/2021 16:13:33 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 16:13:33 - INFO - __main__ -     map = 0.7530377533502448\n","08/09/2021 16:13:35 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_1\n","08/09/2021 16:21:05 - INFO - __main__ -   Iter = 1500\n","08/09/2021 16:21:05 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/09/2021 16:21:05 - INFO - __main__ -   loss = 0.20468663153549035\n","08/09/2021 16:21:05 - INFO - __main__ -   Current data iter size: 1740\n","08/09/2021 16:21:05 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 16:21:21 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 16:21:21 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 16:21:21 - INFO - __main__ -     Batch size = 64\n","08/09/2021 16:29:01 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 16:29:01 - INFO - __main__ -     map = 0.7450395666303078\n","08/09/2021 16:36:31 - INFO - __main__ -   Iter = 1800\n","08/09/2021 16:36:31 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/09/2021 16:36:31 - INFO - __main__ -   loss = 0.27498325179020566\n","08/09/2021 16:36:31 - INFO - __main__ -   Current data iter size: 2009\n","08/09/2021 16:36:31 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 16:36:47 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 16:36:47 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 16:36:47 - INFO - __main__ -     Batch size = 64\n","08/09/2021 16:44:27 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 16:44:27 - INFO - __main__ -     map = 0.7601095802712122\n","08/09/2021 16:44:29 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_1\n","08/09/2021 16:51:59 - INFO - __main__ -   Iter = 2100\n","08/09/2021 16:51:59 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/09/2021 16:51:59 - INFO - __main__ -   loss = 0.3676547030111154\n","08/09/2021 16:51:59 - INFO - __main__ -   Current data iter size: 2320\n","08/09/2021 16:51:59 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 16:52:16 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 16:52:16 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 16:52:16 - INFO - __main__ -     Batch size = 64\n","08/09/2021 16:59:55 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 16:59:55 - INFO - __main__ -     map = 0.761232954819498\n","08/09/2021 16:59:57 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_1\n","08/09/2021 17:07:27 - INFO - __main__ -   Iter = 2400\n","08/09/2021 17:07:27 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/09/2021 17:07:27 - INFO - __main__ -   loss = 0.453542489806811\n","08/09/2021 17:07:27 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 17:07:27 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 17:07:44 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 17:07:44 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 17:07:44 - INFO - __main__ -     Batch size = 64\n","08/09/2021 17:15:23 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 17:15:23 - INFO - __main__ -     map = 0.7682548340748114\n","08/09/2021 17:15:25 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_1\n","08/09/2021 17:19:43 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/09/2021 17:19:43 - INFO - __main__ -    global_step = 2572, average loss = 0.24893455705272316\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3ajxPseo8EJ","outputId":"6df765e6-1597-4e84-f59d-f86495f18165"},"source":["# seed 1 1h43m\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 2\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_2_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function geom_progression\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/10/2021 20:02:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/10/2021 20:02:39 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpva9kxzqq\n","100% 433/433 [00:00<00:00, 402869.04B/s]\n","08/10/2021 20:02:39 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpva9kxzqq to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:02:39 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:02:39 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpva9kxzqq\n","08/10/2021 20:02:39 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:02:39 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/10/2021 20:02:39 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp9krmrvyj\n","100% 231508/231508 [00:00<00:00, 933913.35B/s]\n","08/10/2021 20:02:40 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp9krmrvyj to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:02:40 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:02:40 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp9krmrvyj\n","08/10/2021 20:02:40 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:02:40 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp8kithjki\n","100% 440473133/440473133 [00:12<00:00, 36526962.13B/s]\n","08/10/2021 20:02:53 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp8kithjki to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:02:54 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:02:54 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp8kithjki\n","08/10/2021 20:02:54 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:02:57 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/10/2021 20:02:57 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/10/2021 20:03:09 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_2_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='geom_progression', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_2', save_aps=False, save_steps=5000, seed=2, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/10/2021 20:03:09 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/10/2021 20:03:29 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/10/2021 20:03:32 - INFO - __main__ -   ***** Running training *****\n","08/10/2021 20:03:32 - INFO - __main__ -     Num examples = 164544\n","08/10/2021 20:03:32 - INFO - __main__ -     Num Epochs = 1\n","08/10/2021 20:03:32 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/10/2021 20:03:32 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/10/2021 20:03:32 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/10/2021 20:03:32 - INFO - __main__ -     Total optimization steps = 2571\n","08/10/2021 20:03:32 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/10/2021 20:03:32 - INFO - __main__ -     data_loaders = [('pacing_function_geom_progression', <torch.utils.data.dataloader.DataLoader object at 0x7fa86cc25890>)]\n","08/10/2021 20:03:32 - INFO - __main__ -   Starting epoch 1\n","08/10/2021 20:03:32 - INFO - __main__ -   Training with pacing_function_geom_progression\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/10/2021 20:11:20 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/10/2021 20:11:20 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/10/2021 20:11:20 - INFO - __main__ -   loss = 0.20183934443940718\n","08/10/2021 20:11:20 - INFO - __main__ -   Current data iter size: 980\n","08/10/2021 20:11:20 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:11:41 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:11:41 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:11:41 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:19:54 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:19:54 - INFO - __main__ -     map = 0.7048234202288091\n","08/10/2021 20:19:55 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_2\n","08/10/2021 20:27:49 - INFO - __main__ -   Iter = 600.0\n","08/10/2021 20:27:49 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/10/2021 20:27:49 - INFO - __main__ -   loss = 0.123895263094455\n","08/10/2021 20:27:49 - INFO - __main__ -   Current data iter size: 1131\n","08/10/2021 20:27:49 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:28:06 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:28:06 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:28:06 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:36:19 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:36:19 - INFO - __main__ -     map = 0.7305309129883809\n","08/10/2021 20:36:21 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_2\n","08/10/2021 20:44:15 - INFO - __main__ -   Iter = 900.0\n","08/10/2021 20:44:15 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/10/2021 20:44:15 - INFO - __main__ -   loss = 0.13546210930372277\n","08/10/2021 20:44:15 - INFO - __main__ -   Current data iter size: 1306\n","08/10/2021 20:44:15 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:44:32 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:44:32 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:44:32 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:52:46 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:52:46 - INFO - __main__ -     map = 0.7431953432801978\n","08/10/2021 20:52:48 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_2\n","08/10/2021 21:00:42 - INFO - __main__ -   Iter = 1200.0\n","08/10/2021 21:00:42 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/10/2021 21:00:42 - INFO - __main__ -   loss = 0.1653549039363861\n","08/10/2021 21:00:42 - INFO - __main__ -   Current data iter size: 1507\n","08/10/2021 21:00:42 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:00:59 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:00:59 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:00:59 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:09:14 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:09:14 - INFO - __main__ -     map = 0.7481076174779354\n","08/10/2021 21:09:15 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_2\n","08/10/2021 21:17:10 - INFO - __main__ -   Iter = 1500.0\n","08/10/2021 21:17:10 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/10/2021 21:17:10 - INFO - __main__ -   loss = 0.21126137192050615\n","08/10/2021 21:17:10 - INFO - __main__ -   Current data iter size: 1740\n","08/10/2021 21:17:10 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:17:27 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:17:27 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:17:27 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:25:41 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:25:41 - INFO - __main__ -     map = 0.744296941019573\n","08/10/2021 21:33:36 - INFO - __main__ -   Iter = 1800.0\n","08/10/2021 21:33:36 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/10/2021 21:33:36 - INFO - __main__ -   loss = 0.28500807454188665\n","08/10/2021 21:33:36 - INFO - __main__ -   Current data iter size: 2009\n","08/10/2021 21:33:36 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:33:53 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:33:53 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:33:53 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:42:07 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:42:07 - INFO - __main__ -     map = 0.755658182966331\n","08/10/2021 21:42:09 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_2\n","08/10/2021 21:50:04 - INFO - __main__ -   Iter = 2100.0\n","08/10/2021 21:50:04 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/10/2021 21:50:04 - INFO - __main__ -   loss = 0.3793615545829137\n","08/10/2021 21:50:04 - INFO - __main__ -   Current data iter size: 2320\n","08/10/2021 21:50:04 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:50:21 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:50:21 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:50:21 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:58:35 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:58:35 - INFO - __main__ -     map = 0.7580118841678295\n","08/10/2021 21:58:36 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cQNTNYD9Qfo-","executionInfo":{"status":"ok","timestamp":1628680813695,"user_tz":-120,"elapsed":7903307,"user":{"displayName":"长安故里","photoUrl":"","userId":"04762263270908229891"}},"outputId":"bcc4d92c-6e1b-44f8-9815-040bab720c20"},"source":["# seed 1 1h43m\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 3\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_3_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function geom_progression\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/11/2021 09:08:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/11/2021 09:08:36 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpxpw7vumy\n","100% 433/433 [00:00<00:00, 442096.79B/s]\n","08/11/2021 09:08:36 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpxpw7vumy to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 09:08:36 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 09:08:36 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpxpw7vumy\n","08/11/2021 09:08:36 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 09:08:36 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/11/2021 09:08:36 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp_j12crjq\n","100% 231508/231508 [00:00<00:00, 846270.44B/s]\n","08/11/2021 09:08:37 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp_j12crjq to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 09:08:37 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 09:08:37 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp_j12crjq\n","08/11/2021 09:08:37 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 09:08:37 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpi123u0f6\n","100% 440473133/440473133 [00:13<00:00, 33469208.57B/s]\n","08/11/2021 09:08:51 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpi123u0f6 to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 09:08:52 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 09:08:52 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpi123u0f6\n","08/11/2021 09:08:52 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 09:08:55 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/11/2021 09:08:55 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/11/2021 09:09:07 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_3_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='geom_progression', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_3', save_aps=False, save_steps=5000, seed=3, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/11/2021 09:09:07 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/11/2021 09:09:28 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/11/2021 09:09:31 - INFO - __main__ -   ***** Running training *****\n","08/11/2021 09:09:31 - INFO - __main__ -     Num examples = 164544\n","08/11/2021 09:09:31 - INFO - __main__ -     Num Epochs = 1\n","08/11/2021 09:09:31 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/11/2021 09:09:31 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/11/2021 09:09:31 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/11/2021 09:09:31 - INFO - __main__ -     Total optimization steps = 2571\n","08/11/2021 09:09:31 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/11/2021 09:09:31 - INFO - __main__ -     data_loaders = [('pacing_function_geom_progression', <torch.utils.data.dataloader.DataLoader object at 0x7f83fd9f4f50>)]\n","08/11/2021 09:09:31 - INFO - __main__ -   Starting epoch 1\n","08/11/2021 09:09:31 - INFO - __main__ -   Training with pacing_function_geom_progression\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/11/2021 09:16:56 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/11/2021 09:16:56 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/11/2021 09:16:56 - INFO - __main__ -   loss = 0.1985773469756047\n","08/11/2021 09:16:56 - INFO - __main__ -   Current data iter size: 980\n","08/11/2021 09:16:56 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 09:17:17 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 09:17:17 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 09:17:17 - INFO - __main__ -     Batch size = 64\n","08/11/2021 09:25:09 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 09:25:09 - INFO - __main__ -     map = 0.7180553635893034\n","08/11/2021 09:25:10 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_3\n","08/11/2021 09:32:48 - INFO - __main__ -   Iter = 600.0\n","08/11/2021 09:32:48 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/11/2021 09:32:48 - INFO - __main__ -   loss = 0.11341669580278298\n","08/11/2021 09:32:48 - INFO - __main__ -   Current data iter size: 1131\n","08/11/2021 09:32:48 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 09:33:04 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 09:33:04 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 09:33:04 - INFO - __main__ -     Batch size = 64\n","08/11/2021 09:40:56 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 09:40:56 - INFO - __main__ -     map = 0.7316038146769118\n","08/11/2021 09:40:58 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_3\n","08/11/2021 09:48:35 - INFO - __main__ -   Iter = 900.0\n","08/11/2021 09:48:35 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/11/2021 09:48:35 - INFO - __main__ -   loss = 0.12021561212992916\n","08/11/2021 09:48:35 - INFO - __main__ -   Current data iter size: 1306\n","08/11/2021 09:48:35 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 09:48:52 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 09:48:52 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 09:48:52 - INFO - __main__ -     Batch size = 64\n","08/11/2021 09:56:43 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 09:56:43 - INFO - __main__ -     map = 0.7494830478881975\n","08/11/2021 09:56:45 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_3\n","08/11/2021 10:04:23 - INFO - __main__ -   Iter = 1200.0\n","08/11/2021 10:04:23 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/11/2021 10:04:23 - INFO - __main__ -   loss = 0.14940422219534716\n","08/11/2021 10:04:23 - INFO - __main__ -   Current data iter size: 1507\n","08/11/2021 10:04:23 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 10:04:40 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 10:04:40 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 10:04:40 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:12:32 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:12:32 - INFO - __main__ -     map = 0.7491717921984962\n","08/11/2021 10:20:10 - INFO - __main__ -   Iter = 1500.0\n","08/11/2021 10:20:10 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/11/2021 10:20:10 - INFO - __main__ -   loss = 0.20407193688054878\n","08/11/2021 10:20:10 - INFO - __main__ -   Current data iter size: 1740\n","08/11/2021 10:20:10 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 10:20:27 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 10:20:27 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 10:20:27 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:28:19 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:28:19 - INFO - __main__ -     map = 0.7558153592523944\n","08/11/2021 10:28:20 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_3\n","08/11/2021 10:35:59 - INFO - __main__ -   Iter = 1800.0\n","08/11/2021 10:35:59 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/11/2021 10:35:59 - INFO - __main__ -   loss = 0.2722846979399522\n","08/11/2021 10:35:59 - INFO - __main__ -   Current data iter size: 2009\n","08/11/2021 10:35:59 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 10:36:16 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 10:36:16 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 10:36:16 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:44:08 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:44:08 - INFO - __main__ -     map = 0.7566450116509604\n","08/11/2021 10:44:10 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_3\n","08/11/2021 10:51:49 - INFO - __main__ -   Iter = 2100.0\n","08/11/2021 10:51:49 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/11/2021 10:51:49 - INFO - __main__ -   loss = 0.3708876335124175\n","08/11/2021 10:51:49 - INFO - __main__ -   Current data iter size: 2320\n","08/11/2021 10:51:49 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 10:52:06 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 10:52:06 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 10:52:06 - INFO - __main__ -     Batch size = 64\n","08/11/2021 10:59:58 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 10:59:58 - INFO - __main__ -     map = 0.7634095932136348\n","08/11/2021 10:59:59 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_geom_progression_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3geom_progression_seed_3\n","08/11/2021 11:07:38 - INFO - __main__ -   Iter = 2400.0\n","08/11/2021 11:07:38 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/11/2021 11:07:38 - INFO - __main__ -   loss = 0.4504110405842463\n","08/11/2021 11:07:38 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 11:07:38 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 11:07:56 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 11:07:56 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 11:07:56 - INFO - __main__ -     Batch size = 64\n","08/11/2021 11:15:48 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 11:15:48 - INFO - __main__ -     map = 0.7614997805936182\n","08/11/2021 11:20:11 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/11/2021 11:20:11 - INFO - __main__ -    global_step = 2572.0, average loss = 0.2514813920179915\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LJzL3IYCn2tw"},"source":["# Mantis linear"]},{"cell_type":"code","metadata":{"id":"gq0iRuMiKKq7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628525038237,"user_tz":-120,"elapsed":8300073,"user":{"displayName":"xy GG","photoUrl":"","userId":"06169222138223251748"}},"outputId":"f3ca45b3-e2e3-4540-904e-23bd3d26cf56"},"source":["# seed 1\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 1\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_linear_seed_1_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function linear\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/09/2021 13:45:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/09/2021 13:45:43 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp07d0w216\n","100% 433/433 [00:00<00:00, 435732.64B/s]\n","08/09/2021 13:45:43 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp07d0w216 to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 13:45:43 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 13:45:43 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp07d0w216\n","08/09/2021 13:45:43 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 13:45:43 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/09/2021 13:45:44 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpivioku5g\n","100% 231508/231508 [00:00<00:00, 863687.89B/s]\n","08/09/2021 13:45:44 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpivioku5g to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 13:45:44 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 13:45:44 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpivioku5g\n","08/09/2021 13:45:44 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 13:45:45 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp0z7urj6p\n","100% 440473133/440473133 [00:12<00:00, 36217228.21B/s]\n","08/09/2021 13:45:57 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp0z7urj6p to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 13:45:58 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 13:45:58 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp0z7urj6p\n","08/09/2021 13:45:59 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 13:46:01 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/09/2021 13:46:01 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/09/2021 13:46:14 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_linear_seed_1_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='linear', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_1', save_aps=False, save_steps=5000, seed=1, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/09/2021 13:46:14 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/09/2021 13:46:34 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/09/2021 13:46:36 - INFO - __main__ -   ***** Running training *****\n","08/09/2021 13:46:36 - INFO - __main__ -     Num examples = 164544\n","08/09/2021 13:46:36 - INFO - __main__ -     Num Epochs = 1\n","08/09/2021 13:46:36 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/09/2021 13:46:36 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/09/2021 13:46:36 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/09/2021 13:46:36 - INFO - __main__ -     Total optimization steps = 2571\n","08/09/2021 13:46:36 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/09/2021 13:46:36 - INFO - __main__ -     data_loaders = [('pacing_function_linear', <torch.utils.data.dataloader.DataLoader object at 0x7f63c80a7890>)]\n","08/09/2021 13:46:36 - INFO - __main__ -   Starting epoch 1\n","08/09/2021 13:46:36 - INFO - __main__ -   Training with pacing_function_linear\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/09/2021 13:54:31 - INFO - __main__ -   Iter = 300\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/09/2021 13:54:31 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/09/2021 13:54:31 - INFO - __main__ -   loss = 0.1969714648152391\n","08/09/2021 13:54:31 - INFO - __main__ -   Current data iter size: 1072\n","08/09/2021 13:54:31 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 13:54:50 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 13:54:50 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 13:54:50 - INFO - __main__ -     Batch size = 64\n","08/09/2021 14:03:08 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 14:03:08 - INFO - __main__ -     map = 0.732745422392917\n","08/09/2021 14:03:10 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_1\n","08/09/2021 14:11:09 - INFO - __main__ -   Iter = 600\n","08/09/2021 14:11:09 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/09/2021 14:11:09 - INFO - __main__ -   loss = 0.1422335703174273\n","08/09/2021 14:11:09 - INFO - __main__ -   Current data iter size: 1295\n","08/09/2021 14:11:09 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:11:26 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:11:26 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:11:26 - INFO - __main__ -     Batch size = 64\n","08/09/2021 14:19:44 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 14:19:44 - INFO - __main__ -     map = 0.747717011166589\n","08/09/2021 14:19:46 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_1\n","08/09/2021 14:27:45 - INFO - __main__ -   Iter = 900\n","08/09/2021 14:27:45 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/09/2021 14:27:45 - INFO - __main__ -   loss = 0.1583388353139162\n","08/09/2021 14:27:45 - INFO - __main__ -   Current data iter size: 1518\n","08/09/2021 14:27:45 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:28:01 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:28:01 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:28:01 - INFO - __main__ -     Batch size = 64\n","08/09/2021 14:36:18 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 14:36:18 - INFO - __main__ -     map = 0.7543752819955234\n","08/09/2021 14:36:20 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_1\n","08/09/2021 14:44:19 - INFO - __main__ -   Iter = 1200\n","08/09/2021 14:44:19 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/09/2021 14:44:19 - INFO - __main__ -   loss = 0.21193248890340327\n","08/09/2021 14:44:19 - INFO - __main__ -   Current data iter size: 1742\n","08/09/2021 14:44:19 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 14:44:37 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 14:44:37 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 14:44:37 - INFO - __main__ -     Batch size = 64\n","08/09/2021 14:52:56 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 14:52:56 - INFO - __main__ -     map = 0.7582150556801734\n","08/09/2021 14:52:57 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_1\n","08/09/2021 15:00:57 - INFO - __main__ -   Iter = 1500\n","08/09/2021 15:00:57 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/09/2021 15:00:57 - INFO - __main__ -   loss = 0.27779213843246303\n","08/09/2021 15:00:57 - INFO - __main__ -   Current data iter size: 1965\n","08/09/2021 15:00:57 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:01:13 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:01:13 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:01:13 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:09:31 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:09:31 - INFO - __main__ -     map = 0.7555081489399861\n","08/09/2021 15:17:31 - INFO - __main__ -   Iter = 1800\n","08/09/2021 15:17:31 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/09/2021 15:17:31 - INFO - __main__ -   loss = 0.3480728347102801\n","08/09/2021 15:17:31 - INFO - __main__ -   Current data iter size: 2188\n","08/09/2021 15:17:31 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:17:48 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:17:48 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:17:48 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:26:06 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:26:06 - INFO - __main__ -     map = 0.7631876733768109\n","08/09/2021 15:26:08 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_1\n","08/09/2021 15:34:08 - INFO - __main__ -   Iter = 2100\n","08/09/2021 15:34:08 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/09/2021 15:34:08 - INFO - __main__ -   loss = 0.40184151237209637\n","08/09/2021 15:34:08 - INFO - __main__ -   Current data iter size: 2412\n","08/09/2021 15:34:08 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:34:25 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:34:25 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:34:25 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:42:44 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:42:44 - INFO - __main__ -     map = 0.7625614660075535\n","08/09/2021 15:50:44 - INFO - __main__ -   Iter = 2400\n","08/09/2021 15:50:44 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/09/2021 15:50:44 - INFO - __main__ -   loss = 0.4510851327578227\n","08/09/2021 15:50:44 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 15:50:44 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 15:51:00 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 15:51:00 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 15:51:00 - INFO - __main__ -     Batch size = 64\n","08/09/2021 15:59:18 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 15:59:18 - INFO - __main__ -     map = 0.7708415849098631\n","08/09/2021 15:59:20 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_1\n","08/09/2021 16:03:56 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/09/2021 16:03:56 - INFO - __main__ -    global_step = 2572, average loss = 0.2871535239874459\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xIUlhd83p4lg","executionInfo":{"status":"ok","timestamp":1628634369315,"user_tz":-120,"elapsed":8509564,"user":{"displayName":"长安故里","photoUrl":"","userId":"04762263270908229891"}},"outputId":"f413727d-a11b-4a89-b5d2-82e42bbd1446"},"source":["# seed 1\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 2\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_linear_seed_2_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function linear\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/10/2021 20:04:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/10/2021 20:04:26 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpckgyhcd7\n","100% 433/433 [00:00<00:00, 385672.89B/s]\n","08/10/2021 20:04:26 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpckgyhcd7 to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:04:26 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:04:26 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpckgyhcd7\n","08/10/2021 20:04:26 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:04:26 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/10/2021 20:04:26 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpul1fqc83\n","100% 231508/231508 [00:00<00:00, 945407.62B/s]\n","08/10/2021 20:04:27 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpul1fqc83 to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:04:27 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:04:27 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpul1fqc83\n","08/10/2021 20:04:27 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:04:27 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpay203grx\n","100% 440473133/440473133 [00:12<00:00, 36073521.88B/s]\n","08/10/2021 20:04:40 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpay203grx to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:04:41 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:04:41 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpay203grx\n","08/10/2021 20:04:41 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:04:44 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/10/2021 20:04:44 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/10/2021 20:04:56 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_linear_seed_2_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='linear', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_2', save_aps=False, save_steps=5000, seed=2, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/10/2021 20:04:56 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/10/2021 20:05:17 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/10/2021 20:05:19 - INFO - __main__ -   ***** Running training *****\n","08/10/2021 20:05:19 - INFO - __main__ -     Num examples = 164544\n","08/10/2021 20:05:19 - INFO - __main__ -     Num Epochs = 1\n","08/10/2021 20:05:19 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/10/2021 20:05:19 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/10/2021 20:05:19 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/10/2021 20:05:19 - INFO - __main__ -     Total optimization steps = 2571\n","08/10/2021 20:05:19 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/10/2021 20:05:19 - INFO - __main__ -     data_loaders = [('pacing_function_linear', <torch.utils.data.dataloader.DataLoader object at 0x7f4c7cc93510>)]\n","08/10/2021 20:05:19 - INFO - __main__ -   Starting epoch 1\n","08/10/2021 20:05:19 - INFO - __main__ -   Training with pacing_function_linear\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/10/2021 20:13:18 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/10/2021 20:13:18 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/10/2021 20:13:18 - INFO - __main__ -   loss = 0.2229251685614387\n","08/10/2021 20:13:18 - INFO - __main__ -   Current data iter size: 1072\n","08/10/2021 20:13:18 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:13:39 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:13:39 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:13:39 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:22:21 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:22:21 - INFO - __main__ -     map = 0.7081196026760003\n","08/10/2021 20:22:23 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_2\n","08/10/2021 20:30:42 - INFO - __main__ -   Iter = 600.0\n","08/10/2021 20:30:42 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/10/2021 20:30:42 - INFO - __main__ -   loss = 0.151842432115227\n","08/10/2021 20:30:42 - INFO - __main__ -   Current data iter size: 1295\n","08/10/2021 20:30:42 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:31:00 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:31:00 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:31:00 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:39:42 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:39:42 - INFO - __main__ -     map = 0.7329916818207641\n","08/10/2021 20:39:44 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_2\n","08/10/2021 20:48:04 - INFO - __main__ -   Iter = 900.0\n","08/10/2021 20:48:04 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/10/2021 20:48:04 - INFO - __main__ -   loss = 0.18345876266558966\n","08/10/2021 20:48:04 - INFO - __main__ -   Current data iter size: 1518\n","08/10/2021 20:48:04 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:48:21 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:48:21 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:48:21 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:57:04 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:57:04 - INFO - __main__ -     map = 0.7437316622047616\n","08/10/2021 20:57:06 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_2\n","08/10/2021 21:05:26 - INFO - __main__ -   Iter = 1200.0\n","08/10/2021 21:05:26 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/10/2021 21:05:26 - INFO - __main__ -   loss = 0.22937061859915653\n","08/10/2021 21:05:26 - INFO - __main__ -   Current data iter size: 1742\n","08/10/2021 21:05:26 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:05:45 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:05:45 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:05:45 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:14:28 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:14:28 - INFO - __main__ -     map = 0.7545332448273655\n","08/10/2021 21:14:30 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_2\n","08/10/2021 21:22:50 - INFO - __main__ -   Iter = 1500.0\n","08/10/2021 21:22:50 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/10/2021 21:22:50 - INFO - __main__ -   loss = 0.28045566074550154\n","08/10/2021 21:22:50 - INFO - __main__ -   Current data iter size: 1965\n","08/10/2021 21:22:50 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:23:06 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:23:06 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:23:06 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:31:48 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:31:48 - INFO - __main__ -     map = 0.7477280538028531\n","08/10/2021 21:40:08 - INFO - __main__ -   Iter = 1800.0\n","08/10/2021 21:40:08 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/10/2021 21:40:08 - INFO - __main__ -   loss = 0.35204600969950356\n","08/10/2021 21:40:08 - INFO - __main__ -   Current data iter size: 2188\n","08/10/2021 21:40:08 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:40:25 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:40:25 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:40:25 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:49:07 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:49:07 - INFO - __main__ -     map = 0.7566259999573335\n","08/10/2021 21:49:09 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_2\n","08/10/2021 21:57:30 - INFO - __main__ -   Iter = 2100.0\n","08/10/2021 21:57:30 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/10/2021 21:57:30 - INFO - __main__ -   loss = 0.40446341956655185\n","08/10/2021 21:57:30 - INFO - __main__ -   Current data iter size: 2412\n","08/10/2021 21:57:30 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:57:47 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:57:47 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:57:47 - INFO - __main__ -     Batch size = 64\n","08/10/2021 22:06:29 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 22:06:29 - INFO - __main__ -     map = 0.7589767941598017\n","08/10/2021 22:06:31 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_2\n","08/10/2021 22:14:52 - INFO - __main__ -   Iter = 2400.0\n","08/10/2021 22:14:52 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/10/2021 22:14:52 - INFO - __main__ -   loss = 0.4602115869522095\n","08/10/2021 22:14:52 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 22:14:52 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 22:15:10 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 22:15:10 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 22:15:10 - INFO - __main__ -     Batch size = 64\n","08/10/2021 22:23:51 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 22:23:51 - INFO - __main__ -     map = 0.7663142918794171\n","08/10/2021 22:23:53 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_2\n","Traceback (most recent call last):\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 712, in <module>\n","    main()\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 659, in main\n","    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 227, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 255, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 149, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0nLauz1QA6LQ","executionInfo":{"status":"ok","timestamp":1628689997705,"user_tz":-120,"elapsed":7954344,"user":{"displayName":"王梦琪","photoUrl":"","userId":"17110178610712270742"}},"outputId":"3e3a3fe4-6a35-41b0-bded-a19248652364"},"source":["# seed 1\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 3\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_linear_seed_3_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function linear\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":3,"outputs":[{"output_type":"stream","text":["08/11/2021 11:40:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/11/2021 11:40:49 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpbok5jqb6\n","100% 433/433 [00:00<00:00, 400029.43B/s]\n","08/11/2021 11:40:49 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpbok5jqb6 to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 11:40:49 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 11:40:49 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpbok5jqb6\n","08/11/2021 11:40:49 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 11:40:49 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/11/2021 11:40:49 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpp38jyjuh\n","100% 231508/231508 [00:00<00:00, 929103.98B/s]\n","08/11/2021 11:40:50 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpp38jyjuh to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 11:40:50 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 11:40:50 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpp38jyjuh\n","08/11/2021 11:40:50 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 11:40:50 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpq6ul4rx7\n","100% 440473133/440473133 [00:10<00:00, 43180307.54B/s]\n","08/11/2021 11:41:01 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpq6ul4rx7 to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 11:41:02 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 11:41:02 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpq6ul4rx7\n","08/11/2021 11:41:02 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 11:41:05 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/11/2021 11:41:05 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/11/2021 11:41:16 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_linear_seed_3_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='linear', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_3', save_aps=False, save_steps=5000, seed=3, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/11/2021 11:41:16 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/11/2021 11:41:37 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/11/2021 11:41:40 - INFO - __main__ -   ***** Running training *****\n","08/11/2021 11:41:40 - INFO - __main__ -     Num examples = 164544\n","08/11/2021 11:41:40 - INFO - __main__ -     Num Epochs = 1\n","08/11/2021 11:41:40 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/11/2021 11:41:40 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/11/2021 11:41:40 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/11/2021 11:41:40 - INFO - __main__ -     Total optimization steps = 2571\n","08/11/2021 11:41:40 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/11/2021 11:41:40 - INFO - __main__ -     data_loaders = [('pacing_function_linear', <torch.utils.data.dataloader.DataLoader object at 0x7fce6bff8a10>)]\n","08/11/2021 11:41:40 - INFO - __main__ -   Starting epoch 1\n","08/11/2021 11:41:40 - INFO - __main__ -   Training with pacing_function_linear\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/11/2021 11:49:07 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/11/2021 11:49:07 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/11/2021 11:49:07 - INFO - __main__ -   loss = 0.21522319629167516\n","08/11/2021 11:49:07 - INFO - __main__ -   Current data iter size: 1072\n","08/11/2021 11:49:07 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 11:49:28 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 11:49:28 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 11:49:28 - INFO - __main__ -     Batch size = 64\n","08/11/2021 11:57:21 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 11:57:21 - INFO - __main__ -     map = 0.7332277387340392\n","08/11/2021 11:57:23 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_3\n","08/11/2021 12:05:05 - INFO - __main__ -   Iter = 600.0\n","08/11/2021 12:05:05 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/11/2021 12:05:05 - INFO - __main__ -   loss = 0.14628131444876394\n","08/11/2021 12:05:05 - INFO - __main__ -   Current data iter size: 1295\n","08/11/2021 12:05:05 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 12:05:22 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 12:05:22 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 12:05:22 - INFO - __main__ -     Batch size = 64\n","08/11/2021 12:13:16 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 12:13:16 - INFO - __main__ -     map = 0.743148753100996\n","08/11/2021 12:13:18 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_3\n","08/11/2021 12:21:01 - INFO - __main__ -   Iter = 900.0\n","08/11/2021 12:21:01 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/11/2021 12:21:01 - INFO - __main__ -   loss = 0.1713534175728758\n","08/11/2021 12:21:01 - INFO - __main__ -   Current data iter size: 1518\n","08/11/2021 12:21:01 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 12:21:17 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 12:21:17 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 12:21:17 - INFO - __main__ -     Batch size = 64\n","08/11/2021 12:29:11 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 12:29:11 - INFO - __main__ -     map = 0.7483390100820512\n","08/11/2021 12:29:12 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_3\n","08/11/2021 12:36:55 - INFO - __main__ -   Iter = 1200.0\n","08/11/2021 12:36:55 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/11/2021 12:36:55 - INFO - __main__ -   loss = 0.2188708667953809\n","08/11/2021 12:36:55 - INFO - __main__ -   Current data iter size: 1742\n","08/11/2021 12:36:55 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 12:37:12 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 12:37:12 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 12:37:12 - INFO - __main__ -     Batch size = 64\n","08/11/2021 12:45:05 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 12:45:05 - INFO - __main__ -     map = 0.7532550770831771\n","08/11/2021 12:45:07 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_3\n","08/11/2021 12:52:49 - INFO - __main__ -   Iter = 1500.0\n","08/11/2021 12:52:49 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/11/2021 12:52:49 - INFO - __main__ -   loss = 0.2741276698807875\n","08/11/2021 12:52:49 - INFO - __main__ -   Current data iter size: 1965\n","08/11/2021 12:52:49 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 12:53:06 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 12:53:06 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 12:53:06 - INFO - __main__ -     Batch size = 64\n","08/11/2021 13:00:59 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 13:00:59 - INFO - __main__ -     map = 0.7575463583118227\n","08/11/2021 13:01:01 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_3\n","08/11/2021 13:08:44 - INFO - __main__ -   Iter = 1800.0\n","08/11/2021 13:08:44 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/11/2021 13:08:44 - INFO - __main__ -   loss = 0.34368369350830713\n","08/11/2021 13:08:44 - INFO - __main__ -   Current data iter size: 2188\n","08/11/2021 13:08:44 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 13:09:02 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 13:09:02 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 13:09:02 - INFO - __main__ -     Batch size = 64\n","08/11/2021 13:16:56 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 13:16:56 - INFO - __main__ -     map = 0.7646426372658309\n","08/11/2021 13:16:58 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_3\n","08/11/2021 13:24:41 - INFO - __main__ -   Iter = 2100.0\n","08/11/2021 13:24:41 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/11/2021 13:24:41 - INFO - __main__ -   loss = 0.40895312597354255\n","08/11/2021 13:24:41 - INFO - __main__ -   Current data iter size: 2412\n","08/11/2021 13:24:41 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 13:24:59 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 13:24:59 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 13:24:59 - INFO - __main__ -     Batch size = 64\n","08/11/2021 13:32:52 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 13:32:52 - INFO - __main__ -     map = 0.7700135839923754\n","08/11/2021 13:32:54 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_linear_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3linear_seed_3\n","08/11/2021 13:40:37 - INFO - __main__ -   Iter = 2400.0\n","08/11/2021 13:40:37 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/11/2021 13:40:37 - INFO - __main__ -   loss = 0.4601324103275935\n","08/11/2021 13:40:37 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 13:40:37 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 13:40:55 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 13:40:55 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 13:40:55 - INFO - __main__ -     Batch size = 64\n","08/11/2021 13:48:49 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 13:48:49 - INFO - __main__ -     map = 0.7626909044287503\n","08/11/2021 13:53:15 - INFO - __main__ -   Finished epoch with 2571 iterations.\n","08/11/2021 13:53:15 - INFO - __main__ -    global_step = 2572.0, average loss = 0.2929580492035574\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NiXMLmiGn2zs"},"source":["# Mantis step"]},{"cell_type":"code","metadata":{"id":"ZVGaXvtAp3TD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628551138188,"user_tz":-120,"elapsed":7601829,"user":{"displayName":"长安故里","photoUrl":"","userId":"04762263270908229891"}},"outputId":"27ac376e-5d0a-4f75-8428-3b92bac6c903"},"source":["# seed 1\n","!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 1\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_step_seed_1_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function step\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/09/2021 21:12:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/09/2021 21:12:21 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp9eew441r\n","100% 433/433 [00:00<00:00, 375389.34B/s]\n","08/09/2021 21:12:22 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp9eew441r to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 21:12:22 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 21:12:22 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp9eew441r\n","08/09/2021 21:12:22 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/09/2021 21:12:22 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/09/2021 21:12:22 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpdanhvp0m\n","100% 231508/231508 [00:00<00:00, 849353.53B/s]\n","08/09/2021 21:12:23 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpdanhvp0m to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 21:12:23 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 21:12:23 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpdanhvp0m\n","08/09/2021 21:12:23 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/09/2021 21:12:23 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpq5utge6n\n","100% 440473133/440473133 [00:12<00:00, 34131805.11B/s]\n","08/09/2021 21:12:36 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpq5utge6n to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 21:12:37 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 21:12:37 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpq5utge6n\n","08/09/2021 21:12:37 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/09/2021 21:12:41 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/09/2021 21:12:41 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/09/2021 21:12:53 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_step_seed_1_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='step', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_1', save_aps=False, save_steps=5000, seed=1, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/09/2021 21:12:53 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/09/2021 21:13:15 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/09/2021 21:13:17 - INFO - __main__ -   ***** Running training *****\n","08/09/2021 21:13:17 - INFO - __main__ -     Num examples = 164544\n","08/09/2021 21:13:17 - INFO - __main__ -     Num Epochs = 1\n","08/09/2021 21:13:17 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/09/2021 21:13:17 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/09/2021 21:13:17 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/09/2021 21:13:17 - INFO - __main__ -     Total optimization steps = 2571\n","08/09/2021 21:13:17 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/09/2021 21:13:17 - INFO - __main__ -     data_loaders = [('pacing_function_step', <torch.utils.data.dataloader.DataLoader object at 0x7f5860035510>)]\n","08/09/2021 21:13:17 - INFO - __main__ -   Starting epoch 1\n","08/09/2021 21:13:17 - INFO - __main__ -   Training with pacing_function_step\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/09/2021 21:20:40 - INFO - __main__ -   Iter = 300\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/09/2021 21:20:40 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/09/2021 21:20:40 - INFO - __main__ -   loss = 0.18415923641994594\n","08/09/2021 21:20:40 - INFO - __main__ -   Current data iter size: 849\n","08/09/2021 21:20:40 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 21:21:00 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 21:21:00 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 21:21:00 - INFO - __main__ -     Batch size = 64\n","08/09/2021 21:28:37 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 21:28:37 - INFO - __main__ -     map = 0.6935944079906735\n","08/09/2021 21:28:39 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_1\n","08/09/2021 21:36:03 - INFO - __main__ -   Iter = 600\n","08/09/2021 21:36:03 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/09/2021 21:36:03 - INFO - __main__ -   loss = 0.06871749495776991\n","08/09/2021 21:36:03 - INFO - __main__ -   Current data iter size: 849\n","08/09/2021 21:36:03 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 21:36:21 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 21:36:21 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 21:36:21 - INFO - __main__ -     Batch size = 64\n","08/09/2021 21:43:58 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 21:43:58 - INFO - __main__ -     map = 0.7150576238524364\n","08/09/2021 21:44:00 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_1\n","08/09/2021 21:51:24 - INFO - __main__ -   Iter = 900\n","08/09/2021 21:51:24 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/09/2021 21:51:24 - INFO - __main__ -   loss = 0.17180061448869915\n","08/09/2021 21:51:24 - INFO - __main__ -   Current data iter size: 1697\n","08/09/2021 21:51:24 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 21:51:41 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 21:51:41 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 21:51:41 - INFO - __main__ -     Batch size = 64\n","08/09/2021 21:59:19 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 21:59:19 - INFO - __main__ -     map = 0.7480310505223079\n","08/09/2021 21:59:20 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_1\n","08/09/2021 22:06:46 - INFO - __main__ -   Iter = 1200\n","08/09/2021 22:06:46 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/09/2021 22:06:46 - INFO - __main__ -   loss = 0.2596319785962502\n","08/09/2021 22:06:46 - INFO - __main__ -   Current data iter size: 1697\n","08/09/2021 22:06:46 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 22:07:04 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 22:07:04 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 22:07:04 - INFO - __main__ -     Batch size = 64\n","08/09/2021 22:14:41 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 22:14:41 - INFO - __main__ -     map = 0.7532004047691865\n","08/09/2021 22:14:44 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_1\n","08/09/2021 22:22:09 - INFO - __main__ -   Iter = 1500\n","08/09/2021 22:22:09 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/09/2021 22:22:09 - INFO - __main__ -   loss = 0.23205110299090545\n","08/09/2021 22:22:09 - INFO - __main__ -   Current data iter size: 1697\n","08/09/2021 22:22:09 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 22:22:25 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 22:22:25 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 22:22:25 - INFO - __main__ -     Batch size = 64\n","08/09/2021 22:30:02 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 22:30:02 - INFO - __main__ -     map = 0.7521335578890977\n","08/09/2021 22:37:29 - INFO - __main__ -   Iter = 1800\n","08/09/2021 22:37:29 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/09/2021 22:37:29 - INFO - __main__ -   loss = 0.4707783397038778\n","08/09/2021 22:37:29 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 22:37:29 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 22:37:46 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 22:37:46 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 22:37:46 - INFO - __main__ -     Batch size = 64\n","08/09/2021 22:45:23 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 22:45:23 - INFO - __main__ -     map = 0.7623132933672195\n","08/09/2021 22:45:25 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_1\n","08/09/2021 22:52:52 - INFO - __main__ -   Iter = 2100\n","08/09/2021 22:52:52 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/09/2021 22:52:52 - INFO - __main__ -   loss = 0.48042456557353336\n","08/09/2021 22:52:52 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 22:52:52 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 22:53:08 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 22:53:08 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 22:53:08 - INFO - __main__ -     Batch size = 64\n","08/09/2021 23:00:46 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 23:00:46 - INFO - __main__ -     map = 0.7637100248851779\n","08/09/2021 23:00:47 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_1\n","08/09/2021 23:08:15 - INFO - __main__ -   Iter = 2400\n","08/09/2021 23:08:15 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/09/2021 23:08:15 - INFO - __main__ -   loss = 0.4727735675374667\n","08/09/2021 23:08:15 - INFO - __main__ -   Current data iter size: 2571\n","08/09/2021 23:08:15 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/09/2021 23:08:32 - INFO - __main__ -   ***** Running evaluation  *****\n","08/09/2021 23:08:32 - INFO - __main__ -     Num examples = 60000\n","08/09/2021 23:08:32 - INFO - __main__ -     Batch size = 64\n","08/09/2021 23:16:10 - INFO - __main__ -   ***** Eval results  *****\n","08/09/2021 23:16:10 - INFO - __main__ -     map = 0.7672816431419606\n","08/09/2021 23:16:11 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_1_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_1\n","Traceback (most recent call last):\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 704, in <module>\n","    main()\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 651, in main\n","    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 227, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 255, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 149, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UROhOKMHrAYg","executionInfo":{"status":"ok","timestamp":1628634233600,"user_tz":-120,"elapsed":8254071,"user":{"displayName":"xy GG","photoUrl":"","userId":"06169222138223251748"}},"outputId":"b7908d7f-fb0a-4586-b9d1-455464883b96"},"source":["!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 2\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_step_seed_2_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function step\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/10/2021 20:06:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/10/2021 20:06:26 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpa2auc8l6\n","100% 433/433 [00:00<00:00, 488339.24B/s]\n","08/10/2021 20:06:26 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpa2auc8l6 to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:06:26 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:06:26 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpa2auc8l6\n","08/10/2021 20:06:26 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/10/2021 20:06:26 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/10/2021 20:06:26 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpoui2uf6m\n","100% 231508/231508 [00:00<00:00, 954621.22B/s]\n","08/10/2021 20:06:27 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpoui2uf6m to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:06:27 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:06:27 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpoui2uf6m\n","08/10/2021 20:06:27 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/10/2021 20:06:27 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpo0xlu9df\n","100% 440473133/440473133 [00:11<00:00, 38256474.50B/s]\n","08/10/2021 20:06:39 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpo0xlu9df to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:06:40 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:06:40 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpo0xlu9df\n","08/10/2021 20:06:40 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/10/2021 20:06:43 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/10/2021 20:06:43 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/10/2021 20:06:55 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_step_seed_2_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='step', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_2', save_aps=False, save_steps=5000, seed=2, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/10/2021 20:06:55 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/10/2021 20:07:18 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/10/2021 20:07:20 - INFO - __main__ -   ***** Running training *****\n","08/10/2021 20:07:20 - INFO - __main__ -     Num examples = 164544\n","08/10/2021 20:07:20 - INFO - __main__ -     Num Epochs = 1\n","08/10/2021 20:07:20 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/10/2021 20:07:20 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/10/2021 20:07:20 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/10/2021 20:07:20 - INFO - __main__ -     Total optimization steps = 2571\n","08/10/2021 20:07:20 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/10/2021 20:07:20 - INFO - __main__ -     data_loaders = [('pacing_function_step', <torch.utils.data.dataloader.DataLoader object at 0x7f0150192a10>)]\n","08/10/2021 20:07:21 - INFO - __main__ -   Starting epoch 1\n","08/10/2021 20:07:21 - INFO - __main__ -   Training with pacing_function_step\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/10/2021 20:15:12 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/10/2021 20:15:12 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/10/2021 20:15:12 - INFO - __main__ -   loss = 0.18737746708715955\n","08/10/2021 20:15:12 - INFO - __main__ -   Current data iter size: 849\n","08/10/2021 20:15:12 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:15:33 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:15:33 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:15:33 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:23:52 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:23:52 - INFO - __main__ -     map = 0.6932428486993487\n","08/10/2021 20:23:54 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_2\n","08/10/2021 20:31:52 - INFO - __main__ -   Iter = 600.0\n","08/10/2021 20:31:52 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/10/2021 20:31:52 - INFO - __main__ -   loss = 0.062367893712750325\n","08/10/2021 20:31:52 - INFO - __main__ -   Current data iter size: 849\n","08/10/2021 20:31:52 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:32:10 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:32:10 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:32:10 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:40:29 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:40:29 - INFO - __main__ -     map = 0.7095225332988504\n","08/10/2021 20:40:31 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_2\n","08/10/2021 20:48:31 - INFO - __main__ -   Iter = 900.0\n","08/10/2021 20:48:31 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/10/2021 20:48:31 - INFO - __main__ -   loss = 0.19263214439037257\n","08/10/2021 20:48:31 - INFO - __main__ -   Current data iter size: 1697\n","08/10/2021 20:48:31 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 20:48:47 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 20:48:47 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 20:48:47 - INFO - __main__ -     Batch size = 64\n","08/10/2021 20:57:07 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 20:57:07 - INFO - __main__ -     map = 0.7376534176148815\n","08/10/2021 20:57:09 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_2\n","08/10/2021 21:05:09 - INFO - __main__ -   Iter = 1200.0\n","08/10/2021 21:05:09 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/10/2021 21:05:09 - INFO - __main__ -   loss = 0.2752688711633285\n","08/10/2021 21:05:09 - INFO - __main__ -   Current data iter size: 1697\n","08/10/2021 21:05:09 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:05:26 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:05:26 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:05:26 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:13:44 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:13:44 - INFO - __main__ -     map = 0.7535774275046413\n","08/10/2021 21:13:46 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_2\n","08/10/2021 21:21:46 - INFO - __main__ -   Iter = 1500.0\n","08/10/2021 21:21:46 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/10/2021 21:21:46 - INFO - __main__ -   loss = 0.24231857895851136\n","08/10/2021 21:21:46 - INFO - __main__ -   Current data iter size: 1697\n","08/10/2021 21:21:46 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:22:03 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:22:03 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:22:03 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:30:22 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:30:22 - INFO - __main__ -     map = 0.7402009062432092\n","08/10/2021 21:38:23 - INFO - __main__ -   Iter = 1800.0\n","08/10/2021 21:38:23 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/10/2021 21:38:23 - INFO - __main__ -   loss = 0.4796189886828264\n","08/10/2021 21:38:23 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 21:38:23 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:38:39 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:38:39 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:38:39 - INFO - __main__ -     Batch size = 64\n","08/10/2021 21:46:57 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 21:46:57 - INFO - __main__ -     map = 0.7565649950199996\n","08/10/2021 21:47:03 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_2\n","08/10/2021 21:55:04 - INFO - __main__ -   Iter = 2100.0\n","08/10/2021 21:55:04 - INFO - __main__ -   lr = 3.6639439906651113e-06\n","08/10/2021 21:55:04 - INFO - __main__ -   loss = 0.4880858799815178\n","08/10/2021 21:55:04 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 21:55:04 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 21:55:22 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 21:55:22 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 21:55:22 - INFO - __main__ -     Batch size = 64\n","08/10/2021 22:03:40 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 22:03:40 - INFO - __main__ -     map = 0.7547042586892246\n","08/10/2021 22:11:40 - INFO - __main__ -   Iter = 2400.0\n","08/10/2021 22:11:40 - INFO - __main__ -   lr = 1.3302217036172696e-06\n","08/10/2021 22:11:40 - INFO - __main__ -   loss = 0.48570859024922053\n","08/10/2021 22:11:40 - INFO - __main__ -   Current data iter size: 2571\n","08/10/2021 22:11:40 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/10/2021 22:11:57 - INFO - __main__ -   ***** Running evaluation  *****\n","08/10/2021 22:11:57 - INFO - __main__ -     Num examples = 60000\n","08/10/2021 22:11:57 - INFO - __main__ -     Batch size = 64\n","08/10/2021 22:20:15 - INFO - __main__ -   ***** Eval results  *****\n","08/10/2021 22:20:15 - INFO - __main__ -     map = 0.7607608191386843\n","08/10/2021 22:20:16 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_2_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_2\n","Traceback (most recent call last):\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 712, in <module>\n","    main()\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 659, in main\n","    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","  File \"drive/MyDrive/transformers_cl/run_glue.py\", line 227, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 255, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 149, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xf62bJhB18Y","outputId":"5ee9fbd8-0ac7-4b83-e946-e96754de08e7"},"source":["!python drive/MyDrive/transformers_cl/run_glue.py \\\n","    --model_type bert \\\n","    --model_name_or_path bert-base-uncased \\\n","    --task_name  mantis_10 \\\n","    --do_train \\\n","    --evaluate_during_training \\\n","    --do_lower_case \\\n","    --data_dir drive/MyDrive/Mantis \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 1 \\\n","    --seed 3\\\n","    --output_dir drive/MyDrive/transformers_cl/mantis_10_step_seed_3_8_6w \\\n","    --logging_steps 300 \\\n","    --curriculum_file  drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3 \\\n","    --pacing_function step\\\n","    --use_additive_cl \\\n","    --eval_all_checkpoints \\\n","    --invert_cl_values\\\n","    --eval_subsize 60000\\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["08/11/2021 12:30:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","08/11/2021 12:30:42 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp9kig3tw2\n","100% 433/433 [00:00<00:00, 415971.97B/s]\n","08/11/2021 12:30:43 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp9kig3tw2 to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 12:30:43 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 12:30:43 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp9kig3tw2\n","08/11/2021 12:30:43 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","08/11/2021 12:30:43 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"mantis_10\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","08/11/2021 12:30:43 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpsoe1fvb1\n","100% 231508/231508 [00:00<00:00, 871091.19B/s]\n","08/11/2021 12:30:43 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpsoe1fvb1 to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 12:30:43 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 12:30:43 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpsoe1fvb1\n","08/11/2021 12:30:43 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","08/11/2021 12:30:44 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp4dnjh_6f\n","100% 440473133/440473133 [00:11<00:00, 37579579.36B/s]\n","08/11/2021 12:30:56 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp4dnjh_6f to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 12:30:57 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 12:30:57 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp4dnjh_6f\n","08/11/2021 12:30:57 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","08/11/2021 12:31:01 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","08/11/2021 12:31:01 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","08/11/2021 12:31:12 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', cached_features_file_dir='', config_name='', curriculum_file='drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3', data_dir='drive/MyDrive/Mantis', debug_mode=False, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=True, eval_difficult=False, eval_subsize=60000, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, invert_cl_values=True, learning_rate=2e-05, local_rank=-1, logging_steps=300, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, noise=0.0, num_train_epochs=1.0, output_dir='drive/MyDrive/transformers_cl/mantis_10_step_seed_3_8_6w', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, pacing_function='step', per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, percentage_data_by_epoch=1.0, reset_clf_weights=False, resume_epoch=1.0, run_name='run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_3', save_aps=False, save_steps=5000, seed=3, server_ip='', server_port='', task_name='mantis_10', tokenizer_name='', use_additive_cl=True, warmup_steps=0, weight_decay=0.0)\n","08/11/2021 12:31:12 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_train_bert-base-uncased_128_mantis_10\n","08/11/2021 12:31:33 - INFO - __main__ -   Using curriculum scoring values from file drive/MyDrive/transformers_cl/mantis_10_bert_bm25_8/preds_dif_c_values_3\n","08/11/2021 12:31:35 - INFO - __main__ -   ***** Running training *****\n","08/11/2021 12:31:35 - INFO - __main__ -     Num examples = 164544\n","08/11/2021 12:31:35 - INFO - __main__ -     Num Epochs = 1\n","08/11/2021 12:31:35 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","08/11/2021 12:31:35 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","08/11/2021 12:31:35 - INFO - __main__ -     Gradient Accumulation steps = 1\n","08/11/2021 12:31:35 - INFO - __main__ -     Total optimization steps = 2571\n","08/11/2021 12:31:35 - INFO - __main__ -     percentage by epoch = 1.000000\n","08/11/2021 12:31:35 - INFO - __main__ -     data_loaders = [('pacing_function_step', <torch.utils.data.dataloader.DataLoader object at 0x7fc76ddb4550>)]\n","08/11/2021 12:31:35 - INFO - __main__ -   Starting epoch 1\n","08/11/2021 12:31:35 - INFO - __main__ -   Training with pacing_function_step\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","08/11/2021 12:39:25 - INFO - __main__ -   Iter = 300.0\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","08/11/2021 12:39:25 - INFO - __main__ -   lr = 1.766627771295216e-05\n","08/11/2021 12:39:25 - INFO - __main__ -   loss = 0.16932280995572607\n","08/11/2021 12:39:25 - INFO - __main__ -   Current data iter size: 849\n","08/11/2021 12:39:25 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 12:39:47 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 12:39:47 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 12:39:47 - INFO - __main__ -     Batch size = 64\n","08/11/2021 12:48:02 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 12:48:02 - INFO - __main__ -     map = 0.700006818563048\n","08/11/2021 12:48:04 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_3\n","08/11/2021 12:55:58 - INFO - __main__ -   Iter = 600.0\n","08/11/2021 12:55:58 - INFO - __main__ -   lr = 1.5332555425904317e-05\n","08/11/2021 12:55:58 - INFO - __main__ -   loss = 0.059516848744436475\n","08/11/2021 12:55:58 - INFO - __main__ -   Current data iter size: 849\n","08/11/2021 12:55:58 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 12:56:14 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 12:56:14 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 12:56:14 - INFO - __main__ -     Batch size = 64\n","08/11/2021 13:04:29 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 13:04:29 - INFO - __main__ -     map = 0.7035647311798633\n","08/11/2021 13:04:30 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_3\n","08/11/2021 13:12:25 - INFO - __main__ -   Iter = 900.0\n","08/11/2021 13:12:25 - INFO - __main__ -   lr = 1.2998833138856476e-05\n","08/11/2021 13:12:25 - INFO - __main__ -   loss = 0.18502306790052292\n","08/11/2021 13:12:25 - INFO - __main__ -   Current data iter size: 1697\n","08/11/2021 13:12:25 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 13:12:42 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 13:12:42 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 13:12:42 - INFO - __main__ -     Batch size = 64\n","08/11/2021 13:20:57 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 13:20:57 - INFO - __main__ -     map = 0.7373016912312268\n","08/11/2021 13:20:58 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_3\n","08/11/2021 13:28:53 - INFO - __main__ -   Iter = 1200.0\n","08/11/2021 13:28:53 - INFO - __main__ -   lr = 1.0665110851808636e-05\n","08/11/2021 13:28:53 - INFO - __main__ -   loss = 0.25884259313344954\n","08/11/2021 13:28:53 - INFO - __main__ -   Current data iter size: 1697\n","08/11/2021 13:28:53 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 13:29:10 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 13:29:10 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 13:29:10 - INFO - __main__ -     Batch size = 64\n","08/11/2021 13:37:24 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 13:37:24 - INFO - __main__ -     map = 0.752425718757447\n","08/11/2021 13:37:26 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_3\n","08/11/2021 13:45:21 - INFO - __main__ -   Iter = 1500.0\n","08/11/2021 13:45:21 - INFO - __main__ -   lr = 8.331388564760793e-06\n","08/11/2021 13:45:21 - INFO - __main__ -   loss = 0.2353051590671142\n","08/11/2021 13:45:21 - INFO - __main__ -   Current data iter size: 1697\n","08/11/2021 13:45:21 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 13:45:38 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 13:45:38 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 13:45:38 - INFO - __main__ -     Batch size = 64\n","08/11/2021 13:53:52 - INFO - __main__ -   ***** Eval results  *****\n","08/11/2021 13:53:52 - INFO - __main__ -     map = 0.7550614256618299\n","08/11/2021 13:53:54 - INFO - __main__ -   Saving best model so far to drive/MyDrive/transformers_cl/mantis_10_step_seed_3_8_6w/checkpoint-best_run_cl__bert_bm25_8/preds_dif_c_values_3step_seed_3\n","08/11/2021 14:01:49 - INFO - __main__ -   Iter = 1800.0\n","08/11/2021 14:01:49 - INFO - __main__ -   lr = 5.9976662777129524e-06\n","08/11/2021 14:01:49 - INFO - __main__ -   loss = 0.4750455033282439\n","08/11/2021 14:01:49 - INFO - __main__ -   Current data iter size: 2571\n","08/11/2021 14:01:49 - INFO - __main__ -   Loading features from cached file drive/MyDrive/Mantis/cached_dev_bert-base-uncased_128_mantis_10\n","08/11/2021 14:02:06 - INFO - __main__ -   ***** Running evaluation  *****\n","08/11/2021 14:02:06 - INFO - __main__ -     Num examples = 60000\n","08/11/2021 14:02:06 - INFO - __main__ -     Batch size = 64\n"],"name":"stdout"}]}]}